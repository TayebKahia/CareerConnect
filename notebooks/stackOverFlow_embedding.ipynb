{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'update_technology_abbreviations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_v2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguageHaveWorkedWith\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 86\u001b[0m updated_df, changes \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_technology_abbreviations\u001b[49m(csv_file, target_col)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary of changes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m orig, updated \u001b[38;5;129;01min\u001b[39;00m changes\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'update_technology_abbreviations' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Predefined mapping from technology to abbreviation.\n",
    "# abbreviations = {\n",
    "#     \"Ada\": \"Ada\",\n",
    "#     \"Apex\": \"Apex\",\n",
    "#     \"Assembly\": \"ASM\",  \n",
    "#     \"Bash/Shell (all shells)\": \"Bash\",\n",
    "#     \"C\": \"C\",\n",
    "#     \"C#\": \"C#\",\n",
    "#     \"C++\": \"C++\",\n",
    "#     \"Clojure\": \"Clojure\",\n",
    "#     \"Cobol\": \"COBOL\",\n",
    "#     \"Crystal\": \"Crystal\",\n",
    "#     \"Dart\": \"Dart\",\n",
    "#     \"Delphi\": \"Delphi\",\n",
    "#     \"Elixir\": \"Elixir\",\n",
    "#     \"Erlang\": \"Erlang\",\n",
    "#     \"F#\": \"F#\",\n",
    "#     \"Fortran\": \"Fortran\",\n",
    "#     \"GDScript\": \"GDScript\",\n",
    "#     \"Go\": \"Go\",\n",
    "#     \"Groovy\": \"Groovy\",\n",
    "#     \"Haskell\": \"Haskell\",\n",
    "#     \"HTML/CSS\": \"HTML/CSS\",\n",
    "#     \"Java\": \"Java\",\n",
    "#     \"JavaScript\": \"JS\",\n",
    "#     \"Julia\": \"Julia\",\n",
    "#     \"Kotlin\": \"Kotlin\",\n",
    "#     \"Lisp\": \"Lisp\",\n",
    "#     \"Lua\": \"Lua\",\n",
    "#     \"MATLAB\": \"MATLAB\",\n",
    "#     \"MicroPython\": \"MicroPython\",\n",
    "#     \"Nim\": \"Nim\",\n",
    "#     \"Objective-C\": \"Obj-C\",\n",
    "#     \"OCaml\": \"OCaml\",\n",
    "#     \"Perl\": \"Perl\",\n",
    "#     \"PHP\": \"PHP\",\n",
    "#     \"PowerShell\": \"PowerShell\",\n",
    "#     \"Prolog\": \"Prolog\",\n",
    "#     \"Python\": \"Python\",\n",
    "#     \"R\": \"R\",\n",
    "#     \"Ruby\": \"Ruby\",\n",
    "#     \"Rust\": \"Rust\",\n",
    "#     \"Scala\": \"Scala\",\n",
    "#     \"Solidity\": \"Solidity\",\n",
    "#     \"SQL\": \"SQL\",\n",
    "#     \"Swift\": \"Swift\",\n",
    "#     \"TypeScript\": \"TS\",\n",
    "#     \"VBA\": \"VBA\",\n",
    "#     \"Visual Basic (.Net)\": \"VB.NET\",\n",
    "#     \"Zephyr\": \"Zephyr\",\n",
    "#     \"Zig\": \"Zig\"\n",
    "# }\n",
    "\n",
    "# def update_technology_abbreviations(csv_path, target_column):\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     summary_changes = {}\n",
    "    \n",
    "#     def process_cell(cell):\n",
    "#         if not isinstance(cell, str):\n",
    "#             return cell\n",
    "#         items = [item.strip() for item in cell.split(\";\") if item.strip()]\n",
    "#         new_items = []\n",
    "#         for item in items:\n",
    "#             if re.search(r'\\(.*\\)', item):\n",
    "#                 new_items.append(item)\n",
    "#                 continue\n",
    "#             updated_item = item\n",
    "#             for tech, abbr in abbreviations.items():\n",
    "#                 pattern = re.compile(rf\"^{re.escape(tech)}$\", re.IGNORECASE)\n",
    "#                 if pattern.match(item):\n",
    "#                     updated_item = f\"{tech} ({abbr})\"\n",
    "#                     summary_changes[item] = updated_item\n",
    "#                     break\n",
    "#             new_items.append(updated_item)\n",
    "#         return \"; \".join(new_items)\n",
    "    \n",
    "#     df[target_column] = df[target_column].apply(process_cell)\n",
    "#     return df, summary_changes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"clean_v2.csv\"\n",
    "    target_col = \"LanguageHaveWorkedWith\"\n",
    "    updated_df, changes = update_technology_abbreviations(csv_file, target_col)\n",
    "    print(\"Summary of changes:\")\n",
    "    for orig, updated in changes.items():\n",
    "        print(f\"{orig} -> {updated}\")\n",
    "    updated_df.to_csv(\"clean_v3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total StackOverflow Concepts: 238\n",
      "Concept embeddings saved to stack_concept_embeddings_all-mpnet-base-v2.npy\n",
      "Graph contains 238 nodes and 5 edges.\n",
      "Total candidate phrases generated: 86\n",
      "Total recognized candidate matches: 22\n",
      "Global filtering completed.\n",
      "\n",
      "Globally Filtered Recognized Concepts using nâ€‘gram detection (from StackOverflow data):\n",
      "============================================================\n",
      "Concept: Microsoft SQL Server (DatabaseHaveWorkedWith)\n",
      "    Detected 3-gram: 'microsoft sql server' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: Amazon Web Services (AWS) (PlatformHaveWorkedWith)\n",
      "    Detected 1-gram: 'aws' with similarity 0.81\n",
      "    Detected 3-gram: 'aws proficient machine' with similarity 0.59\n",
      "    Detected 2-gram: 'aws proficient' with similarity 0.57\n",
      "------------------------------------------------------------\n",
      "Concept: Python (Python) (LanguageHaveWorkedWith)\n",
      "    Detected 1-gram: 'python' with similarity 0.81\n",
      "------------------------------------------------------------\n",
      "Concept: Google Cloud (PlatformHaveWorkedWith)\n",
      "    Detected 2-gram: 'cloud computing' with similarity 0.66\n",
      "------------------------------------------------------------\n",
      "Concept: Microsoft Azure (PlatformHaveWorkedWith)\n",
      "    Detected 3-gram: 'cloud computing platforms' with similarity 0.64\n",
      "------------------------------------------------------------\n",
      "Concept: Oracle Cloud Infrastructure (OCI) (PlatformHaveWorkedWith)\n",
      "    Detected 3-gram: 'obj-c cloud computing' with similarity 0.62\n",
      "------------------------------------------------------------\n",
      "Concept: Objective-C (Obj-C) (LanguageHaveWorkedWith)\n",
      "    Detected 2-gram: 'python obj-c' with similarity 0.60\n",
      "    Detected 2-gram: 'obj-c cloud' with similarity 0.56\n",
      "------------------------------------------------------------\n",
      "Concept: PlatformIO (EmbeddedHaveWorkedWith)\n",
      "    Detected 1-gram: 'platforms' with similarity 0.59\n",
      "------------------------------------------------------------\n",
      "Concept: Cloudflare (PlatformHaveWorkedWith)\n",
      "    Detected 1-gram: 'cloud' with similarity 0.55\n",
      "------------------------------------------------------------\n",
      "Concept: VMware (PlatformHaveWorkedWith)\n",
      "    Detected 2-gram: 'computing platforms' with similarity 0.54\n",
      "------------------------------------------------------------\n",
      "Concept: Scikit-Learn (MiscTechHaveWorkedWith)\n",
      "    Detected 2-gram: 'machine learning' with similarity 0.54\n",
      "------------------------------------------------------------\n",
      "Concept: Render (PlatformHaveWorkedWith)\n",
      "    Detected 1-gram: 'graphical' with similarity 0.52\n",
      "------------------------------------------------------------\n",
      "Concept: Databricks (PlatformHaveWorkedWith)\n",
      "    Detected 1-gram: 'data' with similarity 0.50\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure required NLTK resources are available.\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "class ConceptMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path=\"clean_v2_updated.csv\",  # Use updated CSV with abbreviations\n",
    "        columns=None,\n",
    "        model_name=\"all-mpnet-base-v2\",\n",
    "        similarity_threshold_graph=0.7,\n",
    "        ngram_threshold=0.5,\n",
    "        filter_similarity_threshold=0.85\n",
    "    ):\n",
    "        if columns is None:\n",
    "            columns = [\n",
    "                \"LanguageHaveWorkedWith\", \"DatabaseHaveWorkedWith\", \"PlatformHaveWorkedWith\",\n",
    "                \"WebframeHaveWorkedWith\", \"EmbeddedHaveWorkedWith\", \"MiscTechHaveWorkedWith\",\n",
    "                \"ToolsTechHaveWorkedWith\"\n",
    "            ]\n",
    "        self.csv_path = csv_path\n",
    "        self.columns = columns\n",
    "        self.model_name = model_name\n",
    "        self.similarity_threshold_graph = similarity_threshold_graph\n",
    "        self.ngram_threshold = ngram_threshold\n",
    "        self.filter_similarity_threshold = filter_similarity_threshold\n",
    "\n",
    "        # Initialize NLTK stop words and custom filter words.\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.custom_filter_words = {\n",
    "            'additionally', 'also', 'furthermore', 'moreover', 'including', 'like', 'career', 'etc'\n",
    "        }\n",
    "\n",
    "        # Initialize the SentenceTransformer model.\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "        # Placeholders for later processing.\n",
    "        self.stack_concepts = []          # List of concept dictionaries.\n",
    "        self.concept_embeddings = None    # Numpy array of concept embeddings.\n",
    "        self.candidate_phrases = []       # Candidate nâ€‘gram phrases from input text.\n",
    "        self.candidate_embeddings = None  # Numpy array of candidate embeddings.\n",
    "        self.recognized_candidates_ngram = []  # Matched candidates with similarity scores.\n",
    "        self.filtered_by_concept = {}     # Final grouped output after global filtering.\n",
    "        self.graph = None                 # Optional similarity graph.\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Lowercase the text and remove punctuation except for hyphens and parentheses.\n",
    "        \"\"\"\n",
    "        # Preserve '-' and parentheses by removing other punctuation.\n",
    "        punctuation_to_remove = \"\".join(ch for ch in string.punctuation if ch not in \"-()\")\n",
    "        text = text.lower().translate(str.maketrans(\"\", \"\", punctuation_to_remove))\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def is_meaningful(self, phrase):\n",
    "        tokens = [t.lower() for t in word_tokenize(phrase) if t.isalpha()]\n",
    "        if not tokens:\n",
    "            return False\n",
    "        if any(token in self.custom_filter_words for token in tokens):\n",
    "            return False\n",
    "        if len(tokens) == 1 and tokens[0] in self.stop_words:\n",
    "            return False\n",
    "        if tokens and sum(1 for t in tokens if t in self.stop_words) / len(tokens) > 0.5:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def load_concepts(self):\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        concepts_dict = {}\n",
    "        for col in self.columns:\n",
    "            cells = df[col].dropna().tolist()\n",
    "            for cell in cells:\n",
    "                items = [item.strip() for item in cell.split(\";\") if item.strip()]\n",
    "                for item in items:\n",
    "                    concepts_dict[item] = col\n",
    "        self.stack_concepts = [{\"name\": name, \"type\": ctype} for name, ctype in concepts_dict.items()]\n",
    "        print(f\"Total StackOverflow Concepts: {len(self.stack_concepts)}\")\n",
    "\n",
    "    def generate_concept_embeddings(self, save_embeddings=True):\n",
    "        concept_texts = [concept[\"name\"] for concept in self.stack_concepts]\n",
    "        self.concept_embeddings = self.model.encode(concept_texts, convert_to_numpy=True)\n",
    "        if save_embeddings:\n",
    "            filename = f\"stack_concept_embeddings_{self.model_name.replace('/', '_')}.npy\"\n",
    "            np.save(filename, self.concept_embeddings)\n",
    "            print(f\"Concept embeddings saved to {filename}\")\n",
    "\n",
    "    def build_similarity_graph(self):\n",
    "        self.graph = nx.Graph()\n",
    "        concept_texts = [concept[\"name\"] for concept in self.stack_concepts]\n",
    "        for concept in self.stack_concepts:\n",
    "            self.graph.add_node(concept[\"name\"], category=concept[\"type\"])\n",
    "        sim_matrix = cosine_similarity(self.concept_embeddings)\n",
    "        for i in range(len(concept_texts)):\n",
    "            for j in range(i + 1, len(concept_texts)):\n",
    "                if sim_matrix[i][j] >= self.similarity_threshold_graph:\n",
    "                    self.graph.add_edge(concept_texts[i], concept_texts[j], weight=sim_matrix[i][j])\n",
    "        print(f\"Graph contains {len(self.graph.nodes)} nodes and {len(self.graph.edges)} edges.\")\n",
    "\n",
    "    def prepare_candidate_phrases(self, long_text):\n",
    "        cleaned_full_text = self.clean_text(long_text)\n",
    "        tokens_clean = word_tokenize(cleaned_full_text)\n",
    "        candidate_phrases = []\n",
    "        for n in [3, 2, 1]:\n",
    "            for gram in ngrams(tokens_clean, n):\n",
    "                phrase = \" \".join(gram)\n",
    "                if phrase.strip() and self.is_meaningful(phrase):\n",
    "                    candidate_phrases.append(phrase)\n",
    "        self.candidate_phrases = list(set(candidate_phrases))\n",
    "        print(f\"Total candidate phrases generated: {len(self.candidate_phrases)}\")\n",
    "\n",
    "    def vectorized_match_candidates(self):\n",
    "        self.candidate_embeddings = self.model.encode(self.candidate_phrases, convert_to_numpy=True)\n",
    "        similarity_matrix = cosine_similarity(self.candidate_embeddings, self.concept_embeddings)\n",
    "        max_similarities = similarity_matrix.max(axis=1)\n",
    "        max_indices = similarity_matrix.argmax(axis=1)\n",
    "        valid_indices = np.where(max_similarities >= self.ngram_threshold)[0]\n",
    "        self.recognized_candidates_ngram = []\n",
    "        for idx in valid_indices:\n",
    "            max_sim = max_similarities[idx]\n",
    "            max_idx = max_indices[idx]\n",
    "            concept_name = self.stack_concepts[max_idx][\"name\"]\n",
    "            concept_type = self.stack_concepts[max_idx][\"type\"]\n",
    "            phrase = self.candidate_phrases[idx]\n",
    "            n_val = len(phrase.split())\n",
    "            tokens_phrase = phrase.split()\n",
    "            self.recognized_candidates_ngram.append(\n",
    "                (concept_name, concept_type, phrase, max_sim, n_val, tokens_phrase)\n",
    "            )\n",
    "        print(f\"Total recognized candidate matches: {len(self.recognized_candidates_ngram)}\")\n",
    "\n",
    "    def global_filtering(self):\n",
    "        recognized = sorted(self.recognized_candidates_ngram, key=lambda x: x[3], reverse=True)\n",
    "        global_used_words = set()\n",
    "        filtered_candidates = []\n",
    "        for candidate in recognized:\n",
    "            concept_name, concept_type, phrase, score, n_val, tokens_phrase = candidate\n",
    "            if any(token in global_used_words for token in tokens_phrase):\n",
    "                continue\n",
    "            filtered_candidates.append(candidate)\n",
    "            if score > self.filter_similarity_threshold:\n",
    "                global_used_words.update(tokens_phrase)\n",
    "        self.filtered_by_concept = {}\n",
    "        for concept_name, concept_type, phrase, score, n_val, tokens_phrase in filtered_candidates:\n",
    "            self.filtered_by_concept.setdefault(concept_name, {\"type\": concept_type, \"phrases\": []})\n",
    "            self.filtered_by_concept[concept_name][\"phrases\"].append((phrase, score, n_val, tokens_phrase))\n",
    "        print(\"Global filtering completed.\")\n",
    "\n",
    "    def print_results(self):\n",
    "        print(\"\\nGlobally Filtered Recognized Concepts using nâ€‘gram detection (from StackOverflow data):\")\n",
    "        print(\"=\" * 60)\n",
    "        for concept, info in self.filtered_by_concept.items():\n",
    "            concept_type = info[\"type\"]\n",
    "            print(f\"Concept: {concept} ({concept_type})\")\n",
    "            for phrase, score, n_val, tokens_phrase in sorted(info[\"phrases\"], key=lambda x: x[1], reverse=True):\n",
    "                print(f\"    Detected {n_val}-gram: '{phrase}' with similarity {score:.2f}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    matcher = ConceptMatcher(\n",
    "        csv_path=\"clean_v2_updated.csv\",\n",
    "        model_name=\"all-mpnet-base-v2\",\n",
    "        similarity_threshold_graph=0.7,\n",
    "        ngram_threshold=0.5,\n",
    "        filter_similarity_threshold=0.85\n",
    "    )\n",
    "    matcher.load_concepts()\n",
    "    matcher.generate_concept_embeddings()\n",
    "    matcher.build_similarity_graph()\n",
    "    \n",
    "    # Sample text now includes \"Obj-C\" as written.\n",
    "    sample_text = \"\"\"\n",
    "    I have extensive experience in data analysis and have worked with a variety of technologies including Microsoft SQL Server,\n",
    "    Python, Obj-C, cloud computing platforms like AWS, and I am proficient with machine learning techniques. My background also includes\n",
    "    developing user interfaces with modern tools. Additionally, I have hands-on experience with business intelligence and\n",
    "    graphical user interface design.\n",
    "    \"\"\"\n",
    "    matcher.prepare_candidate_phrases(sample_text)\n",
    "    matcher.vectorized_match_candidates()\n",
    "    matcher.global_filtering()\n",
    "    matcher.print_results()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
