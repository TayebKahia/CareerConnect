{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462957c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Schema:\n",
      "- list of dict\n",
      "    code: str\n",
      "    tasks: list\n",
      "        - list of dict\n",
      "            statement: str\n",
      "            category: str\n",
      "            score: dict\n",
      "                scale: str\n",
      "                important: bool\n",
      "                value: int\n",
      "    technology_skills: list\n",
      "        - list of dict\n",
      "            skill_title: str\n",
      "            technologies: list\n",
      "                - list of dict\n",
      "                    name: str\n",
      "                    demand_percentage: str\n",
      "                    in_demand: str\n",
      "                    hot_tech_percentage: str\n",
      "                    hot_tech: str\n",
      "                    hot_tech_in_demand: str\n",
      "    tools_used: list\n",
      "        - list of dict\n",
      "            tool_title: str\n",
      "    knowledge: list\n",
      "        - list of dict\n",
      "            name: str\n",
      "            description: str\n",
      "            score: dict\n",
      "                scale: str\n",
      "                important: bool\n",
      "                value: int\n",
      "    skills: list\n",
      "        - list of dict\n",
      "            name: str\n",
      "            description: str\n",
      "            score: dict\n",
      "                scale: str\n",
      "                important: bool\n",
      "                value: int\n",
      "    abilities: list\n",
      "        - list of dict\n",
      "            name: str\n",
      "            description: str\n",
      "            score: dict\n",
      "                scale: str\n",
      "                important: bool\n",
      "                value: int\n",
      "    work_activities: list\n",
      "        - list of dict\n",
      "            name: str\n",
      "            description: str\n",
      "            score: dict\n",
      "                scale: str\n",
      "                important: bool\n",
      "                value: int\n",
      "    job_zone: dict\n",
      "        value: int\n",
      "        title: str\n",
      "        education: str\n",
      "        related_experience: str\n",
      "        job_training: str\n",
      "        job_zone_examples: str\n",
      "        svp_range: str\n",
      "    interests: list\n",
      "        - list of dict\n",
      "            name: str\n",
      "            description: str\n",
      "            score: dict\n",
      "                scale: str\n",
      "                important: bool\n",
      "                value: int\n",
      "    related_occupations: list\n",
      "        - list of dict\n",
      "            title: str\n",
      "            code: str\n",
      "    title: str\n",
      "    description: str\n",
      "    alternative_titles: list\n",
      "        - list of str\n",
      "    education_level_required: list\n",
      "        - list of dict\n",
      "            name: str\n",
      "            description: str\n",
      "            score: int\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_schema(data, indent=0):\n",
    "    prefix = ' ' * indent\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            print(f\"{prefix}{key}: {type(value).__name__}\")\n",
    "            get_schema(value, indent + 4)\n",
    "    elif isinstance(data, list) and data:\n",
    "        print(f\"{prefix}- list of {type(data[0]).__name__}\")\n",
    "        get_schema(data[0], indent + 4)\n",
    "\n",
    "\n",
    "# Load the JSON file\n",
    "with open('../OnetData/abbr_cleaned_IT_data_from_onet.json', 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Print the schema\n",
    "print(\"JSON Schema:\")\n",
    "get_schema(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e82bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:00<00:00, 116.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load processed concepts and embeddings\n",
    "model_name = \"sentence-transformers/msmarco-distilbert-base-v4\"\n",
    "data = np.load(f\"onet_concept_embeddings_{model_name.replace('/', '_')}.npz\")\n",
    "main_emb = data['main']\n",
    "abbr_emb = data['abbr']\n",
    "\n",
    "with open(f\"processed_onet_concepts_{model_name.replace('/', '_')}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    processed_concepts = json.load(f)\n",
    "\n",
    "# Load job data\n",
    "with open(\"../OnetData/abbr_cleaned_IT_data_from_onet.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jobs = json.load(f)\n",
    "\n",
    "# Precompute job embeddings\n",
    "job_embeddings = []\n",
    "job_titles = []\n",
    "\n",
    "for job in tqdm(jobs):\n",
    "    terms = []\n",
    "\n",
    "    for tech_skill in job.get(\"technology_skills\", []):\n",
    "        skill_title = tech_skill.get(\"skill_title\", \"\")\n",
    "        technologies = tech_skill.get(\"technologies\", [])\n",
    "\n",
    "        # Calculate average demand for skill title\n",
    "        avg_demand = np.mean([float(t.get(\"demand_percentage\", 0))\n",
    "                             for t in technologies]) if technologies else 0.0\n",
    "        terms.append((skill_title, avg_demand))\n",
    "\n",
    "        # Add each technology with its demand\n",
    "        for tech in technologies:\n",
    "            tech_name = tech.get(\"name\", \"\")\n",
    "            tech_demand = float(tech.get(\"demand_percentage\", 0))\n",
    "            terms.append((tech_name, tech_demand))\n",
    "\n",
    "    # Aggregate term embeddings\n",
    "    job_vec = np.zeros_like(main_emb[0])\n",
    "    total_weight = 0.0\n",
    "\n",
    "    for term_name, weight in terms:\n",
    "        concept = next(\n",
    "            (c for c in processed_concepts if c['name'] == term_name), None)\n",
    "        if not concept:\n",
    "            continue\n",
    "\n",
    "        idx = processed_concepts.index(concept)\n",
    "        m_emb = main_emb[idx]\n",
    "        a_emb = abbr_emb[idx]\n",
    "\n",
    "        # Use average of main and abbr if available\n",
    "        if concept['abbr']:\n",
    "            term_vec = (m_emb + a_emb) / 2\n",
    "        else:\n",
    "            term_vec = m_emb\n",
    "\n",
    "        job_vec += term_vec * weight\n",
    "        total_weight += weight\n",
    "\n",
    "    if total_weight > 0:\n",
    "        job_vec /= total_weight\n",
    "\n",
    "    job_embeddings.append(job_vec)\n",
    "    job_titles.append(job.get(\"title\", \"\"))\n",
    "\n",
    "# Save precomputed data\n",
    "np.save(\"job_embeddings.npy\", np.array(job_embeddings))\n",
    "with open(\"job_titles.json\", \"w\") as f:\n",
    "    json.dump(job_titles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae6519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load precomputed data\n",
    "job_embeddings = np.load(\"job_embeddings.npy\")\n",
    "with open(\"job_titles.json\", \"r\") as f:\n",
    "    job_titles = json.load(f)\n",
    "\n",
    "def recommend_jobs(filtered_candidates, top_n=5):\n",
    "    # Generate user embedding\n",
    "    user_vec = np.zeros_like(job_embeddings[0])\n",
    "    total_score = 0.0\n",
    "    \n",
    "    for candidate in filtered_candidates:\n",
    "        concept_name = candidate[0]\n",
    "        score = candidate[3]\n",
    "        \n",
    "        concept = next((c for c in processed_concepts if c['name'] == concept_name), None)\n",
    "        if not concept:\n",
    "            continue\n",
    "        \n",
    "        idx = processed_concepts.index(concept)\n",
    "        m_emb = main_emb[idx]\n",
    "        a_emb = abbr_emb[idx]\n",
    "        \n",
    "        if concept['abbr']:\n",
    "            term_vec = (m_emb + a_emb) / 2\n",
    "        else:\n",
    "            term_vec = m_emb\n",
    "        \n",
    "        user_vec += term_vec * score\n",
    "        total_score += score\n",
    "    \n",
    "    if total_score > 0:\n",
    "        user_vec /= total_score\n",
    "    \n",
    "    # Compute similarities\n",
    "    sims = cosine_similarity([user_vec], job_embeddings)[0]\n",
    "    top_indices = np.argsort(sims)[-top_n:][::-1]\n",
    "    \n",
    "    return [(job_titles[i], sims[i]) for i in top_indices]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
