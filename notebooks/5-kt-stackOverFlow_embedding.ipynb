{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow Technology Embedding and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\deep_learning\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.3 when it was built against 1.14.2, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure required NLTK resources are available.\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "class ConceptMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path=\"../data/processed/technologies_with_abbreviations.csv\",  # Updated path to new CSV with abbreviations\n",
    "        columns=None,\n",
    "        model_name=\"all-mpnet-base-v2\",\n",
    "        similarity_threshold_graph=0.7,\n",
    "        ngram_threshold=0.5,\n",
    "        filter_similarity_threshold=0.85\n",
    "    ):\n",
    "        if columns is None:\n",
    "            columns = [\n",
    "                \"LanguageHaveWorkedWith\", \"DatabaseHaveWorkedWith\", \"PlatformHaveWorkedWith\",\n",
    "                \"WebframeHaveWorkedWith\", \"EmbeddedHaveWorkedWith\", \"MiscTechHaveWorkedWith\",\n",
    "                \"ToolsTechHaveWorkedWith\"\n",
    "            ]\n",
    "        self.csv_path = csv_path\n",
    "        self.columns = columns\n",
    "        self.model_name = model_name\n",
    "        self.similarity_threshold_graph = similarity_threshold_graph\n",
    "        self.ngram_threshold = ngram_threshold\n",
    "        self.filter_similarity_threshold = filter_similarity_threshold\n",
    "\n",
    "        # Initialize NLTK stop words and custom filter words.\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.custom_filter_words = {\n",
    "            'additionally', 'also', 'furthermore', 'moreover', 'including', 'like', 'career', 'etc'\n",
    "        }\n",
    "\n",
    "        # Initialize the SentenceTransformer model.\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "        # Placeholders for later processing.\n",
    "        self.tech_data = None          # DataFrame to store technology names and abbreviations\n",
    "        self.stack_concepts = []       # List of concept dictionaries.\n",
    "        self.concept_embeddings = None # Numpy array of concept embeddings.\n",
    "        self.candidate_phrases = []    # Candidate n‑gram phrases from input text.\n",
    "        self.candidate_embeddings = None  # Numpy array of candidate embeddings.\n",
    "        self.recognized_candidates_ngram = []  # Matched candidates with similarity scores.\n",
    "        self.filtered_by_concept = {}     # Final grouped output after global filtering.\n",
    "        self.graph = None                 # Optional similarity graph.\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Lowercase the text and remove punctuation except for hyphens and parentheses.\n",
    "        \"\"\"\n",
    "        # Preserve '-' and parentheses by removing other punctuation.\n",
    "        punctuation_to_remove = \"\".join(ch for ch in string.punctuation if ch not in \"-()\")\n",
    "        text = text.lower().translate(str.maketrans(\"\", \"\", punctuation_to_remove))\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def is_meaningful(self, phrase):\n",
    "        tokens = [t.lower() for t in word_tokenize(phrase) if t.isalpha()]\n",
    "        if not tokens:\n",
    "            return False\n",
    "        if any(token in self.custom_filter_words for token in tokens):\n",
    "            return False\n",
    "        if len(tokens) == 1 and tokens[0] in self.stop_words:\n",
    "            return False\n",
    "        if tokens and sum(1 for t in tokens if t in self.stop_words) / len(tokens) > 0.5:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def load_concepts(self):\n",
    "        # Load the technology data with abbreviations\n",
    "        self.tech_data = pd.read_csv(self.csv_path)\n",
    "        print(f\"Loaded technologies data with {len(self.tech_data)} entries\")\n",
    "        \n",
    "        # Create concepts from both technology names and their abbreviations\n",
    "        self.stack_concepts = []\n",
    "        \n",
    "        # Add technology names first\n",
    "        for _, row in self.tech_data.iterrows():\n",
    "            tech_name = row['Technology']\n",
    "            self.stack_concepts.append({\"name\": tech_name, \"type\": \"Technology\", \"original\": tech_name})\n",
    "        \n",
    "        # Add abbreviations if they exist\n",
    "        for _, row in self.tech_data.iterrows():\n",
    "            if pd.notna(row['abrv']) and row['abrv'].strip():  # Check if abbreviation exists and is not empty\n",
    "                tech_name = row['Technology']\n",
    "                abbr = row['abrv']\n",
    "                self.stack_concepts.append({\"name\": abbr, \"type\": \"Abbreviation\", \"original\": tech_name})\n",
    "        \n",
    "        print(f\"Total StackOverflow Concepts (including abbreviations): {len(self.stack_concepts)}\")\n",
    "\n",
    "    def generate_concept_embeddings(self, save_embeddings=True, load_if_exists=True):\n",
    "        \"\"\"\n",
    "        Generate embeddings for all technology concepts or load existing ones if available.\n",
    "        \n",
    "        Parameters:\n",
    "        - save_embeddings: Whether to save newly generated embeddings\n",
    "        - load_if_exists: Whether to try loading existing embeddings first\n",
    "        \"\"\"\n",
    "        filename = f\"stack_concept_embeddings_{self.model_name.replace('/', '_')}.npy\"\n",
    "        \n",
    "        # Try to load existing embeddings if requested\n",
    "        if load_if_exists:\n",
    "            try:\n",
    "                self.concept_embeddings = np.load(filename)\n",
    "                print(f\"Loaded existing concept embeddings from {filename}\")\n",
    "                return\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No existing embeddings found at {filename}, generating new ones...\")\n",
    "        \n",
    "        # Generate new embeddings\n",
    "        concept_texts = [concept[\"name\"] for concept in self.stack_concepts]\n",
    "        self.concept_embeddings = self.model.encode(concept_texts, convert_to_numpy=True)\n",
    "        \n",
    "        if save_embeddings:\n",
    "            np.save(filename, self.concept_embeddings)\n",
    "            print(f\"Concept embeddings saved to {filename}\")\n",
    "\n",
    "    def build_similarity_graph(self):\n",
    "        self.graph = nx.Graph()\n",
    "        concept_texts = [concept[\"name\"] for concept in self.stack_concepts]\n",
    "        for concept in self.stack_concepts:\n",
    "            self.graph.add_node(concept[\"name\"], category=concept[\"type\"], original=concept.get(\"original\", concept[\"name\"]))\n",
    "        sim_matrix = cosine_similarity(self.concept_embeddings)\n",
    "        for i in range(len(concept_texts)):\n",
    "            for j in range(i + 1, len(concept_texts)):\n",
    "                if sim_matrix[i][j] >= self.similarity_threshold_graph:\n",
    "                    self.graph.add_edge(concept_texts[i], concept_texts[j], weight=sim_matrix[i][j])\n",
    "        print(f\"Graph contains {len(self.graph.nodes)} nodes and {len(self.graph.edges)} edges.\")\n",
    "\n",
    "    def prepare_candidate_phrases(self, long_text):\n",
    "        cleaned_full_text = self.clean_text(long_text)\n",
    "        tokens_clean = word_tokenize(cleaned_full_text)\n",
    "        candidate_phrases = []\n",
    "        for n in [3, 2, 1]:\n",
    "            for gram in ngrams(tokens_clean, n):\n",
    "                phrase = \" \".join(gram)\n",
    "                if phrase.strip() and self.is_meaningful(phrase):\n",
    "                    candidate_phrases.append(phrase)\n",
    "        self.candidate_phrases = list(set(candidate_phrases))\n",
    "        print(f\"Total candidate phrases generated: {len(self.candidate_phrases)}\")\n",
    "\n",
    "    def vectorized_match_candidates(self):\n",
    "        self.candidate_embeddings = self.model.encode(self.candidate_phrases, convert_to_numpy=True)\n",
    "        similarity_matrix = cosine_similarity(self.candidate_embeddings, self.concept_embeddings)\n",
    "        max_similarities = similarity_matrix.max(axis=1)\n",
    "        max_indices = similarity_matrix.argmax(axis=1)\n",
    "        valid_indices = np.where(max_similarities >= self.ngram_threshold)[0]\n",
    "        self.recognized_candidates_ngram = []\n",
    "        for idx in valid_indices:\n",
    "            max_sim = max_similarities[idx]\n",
    "            max_idx = max_indices[idx]\n",
    "            concept_name = self.stack_concepts[max_idx][\"name\"]\n",
    "            concept_type = self.stack_concepts[max_idx][\"type\"]\n",
    "            original_name = self.stack_concepts[max_idx].get(\"original\", concept_name)\n",
    "            phrase = self.candidate_phrases[idx]\n",
    "            n_val = len(phrase.split())\n",
    "            tokens_phrase = phrase.split()\n",
    "            self.recognized_candidates_ngram.append(\n",
    "                (original_name, concept_type, phrase, max_sim, n_val, tokens_phrase)\n",
    "            )\n",
    "        print(f\"Total recognized candidate matches: {len(self.recognized_candidates_ngram)}\")\n",
    "\n",
    "    def global_filtering(self):\n",
    "        recognized = sorted(self.recognized_candidates_ngram, key=lambda x: x[3], reverse=True)\n",
    "        global_used_words = set()\n",
    "        filtered_candidates = []\n",
    "        for candidate in recognized:\n",
    "            concept_name, concept_type, phrase, score, n_val, tokens_phrase = candidate\n",
    "            if any(token in global_used_words for token in tokens_phrase):\n",
    "                continue\n",
    "            filtered_candidates.append(candidate)\n",
    "            if score > self.filter_similarity_threshold:\n",
    "                global_used_words.update(tokens_phrase)\n",
    "        self.filtered_by_concept = {}\n",
    "        for concept_name, concept_type, phrase, score, n_val, tokens_phrase in filtered_candidates:\n",
    "            self.filtered_by_concept.setdefault(concept_name, {\"type\": concept_type, \"phrases\": []})\n",
    "            self.filtered_by_concept[concept_name][\"phrases\"].append((phrase, score, n_val, tokens_phrase))\n",
    "        print(\"Global filtering completed.\")\n",
    "\n",
    "    def print_results(self):\n",
    "        print(\"\\nGlobally Filtered Recognized Concepts using n‑gram detection (from StackOverflow data):\")\n",
    "        print(\"=\" * 60)\n",
    "        for concept, info in self.filtered_by_concept.items():\n",
    "            concept_type = info[\"type\"]\n",
    "            print(f\"Concept: {concept} ({concept_type})\")\n",
    "            for phrase, score, n_val, tokens_phrase in sorted(info[\"phrases\"], key=lambda x: x[1], reverse=True):\n",
    "                print(f\"    Detected {n_val}-gram: '{phrase}' with similarity {score:.2f}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    def get_recognized_technologies(self):\n",
    "        \"\"\"Get a simple list of recognized technologies.\"\"\"\n",
    "        return list(self.filtered_by_concept.keys())\n",
    "\n",
    "model_names = [\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/msmarco-distilbert-base-v4\",\n",
    "    \"all-mpnet-base-v2\"\n",
    "]\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    matcher = ConceptMatcher(\n",
    "        csv_path=\"../data/processed/technologies_with_abbreviations.csv\",\n",
    "        model_name=\"all-mpnet-base-v2\",\n",
    "        similarity_threshold_graph=0.7,\n",
    "        ngram_threshold=0.5,\n",
    "        filter_similarity_threshold=0.85\n",
    "    )\n",
    "    matcher.load_concepts()\n",
    "    matcher.generate_concept_embeddings(save_embeddings=True, load_if_exists=True)\n",
    "    matcher.build_similarity_graph()\n",
    "    \n",
    "    # Sample text now includes abbreviations that should be detected\n",
    "    sample_text = \"\"\"\n",
    "    I have extensive experience in data analysis and have worked with a variety of technologies including MSSQL,\n",
    "    Py, ObjC, cloud computing platforms like AWS, and I am proficient with machine learning techniques using TF and Sklearn. My background also includes\n",
    "    developing user interfaces with React and Vue. Additionally, I have hands-on experience with business intelligence and\n",
    "    graphical user interface design using Node and TS.\n",
    "    \"\"\"\n",
    "    matcher.prepare_candidate_phrases(sample_text)\n",
    "    matcher.vectorized_match_candidates()\n",
    "    matcher.global_filtering()\n",
    "    matcher.print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career Path Prediction Model Implementation\n",
    "\n",
    "This notebook demonstrates the implementation of an efficient machine learning model for predicting career paths based on technology skills and years of coding experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data\n",
    "def load_data(file_path=\"../data/processed/clean_v3.csv\"):\n",
    "    \"\"\"Load and prepare the Stack Overflow survey data.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded dataset with shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We need to preprocess the data to prepare it for model training. This includes handling the `YearsCode` column properly as a numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if YearsCode column exists\n",
    "if 'YearsCode' in df.columns:\n",
    "    print(\"YearsCode column exists. Checking value distribution...\")\n",
    "    print(df['YearsCode'].value_counts().head(10))\n",
    "    print(f\"Data type: {df['YearsCode'].dtype}\")\n",
    "else:\n",
    "    print(\"YearsCode column not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle YearsCode column - convert to numeric\n",
    "def preprocess_years_code(df):\n",
    "    \"\"\"Convert YearsCode column to numeric values.\"\"\"\n",
    "    if 'YearsCode' in df.columns:\n",
    "        # Make a copy to avoid warning\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Replace text values with numeric equivalents\n",
    "        df['YearsCode'] = df['YearsCode'].replace('Less than 1 year', '0.5')\n",
    "        df['YearsCode'] = df['YearsCode'].replace('More than 50 years', '51')\n",
    "        \n",
    "        # Convert to numeric and handle errors\n",
    "        df['YearsCode'] = pd.to_numeric(df['YearsCode'], errors='coerce')\n",
    "        \n",
    "        # Fill missing values with median\n",
    "        median_years = df['YearsCode'].median()\n",
    "        df['YearsCode'].fillna(median_years, inplace=True)\n",
    "        \n",
    "        print(f\"YearsCode converted to numeric. Median value: {median_years}\")\n",
    "    return df\n",
    "\n",
    "# Preprocess YearsCode column\n",
    "df = preprocess_years_code(df)\n",
    "\n",
    "# Visualize YearsCode distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['YearsCode'], bins=20)\n",
    "plt.title('Distribution of Years of Coding Experience')\n",
    "plt.xlabel('Years of Coding Experience')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the technology columns and target column\n",
    "TECH_COLUMNS = [\n",
    "    \"LanguageHaveWorkedWith\", \n",
    "    \"DatabaseHaveWorkedWith\", \n",
    "    \"PlatformHaveWorkedWith\",\n",
    "    \"WebframeHaveWorkedWith\", \n",
    "    \"MiscTechHaveWorkedWith\",\n",
    "    \"ToolsTechHaveWorkedWith\"\n",
    "]\n",
    "TARGET_COLUMN = \"DevType\"\n",
    "\n",
    "# Process technology columns (convert from semicolon-separated strings to lists)\n",
    "def process_tech_columns(df, tech_columns=TECH_COLUMNS):\n",
    "    \"\"\"Process technology columns by splitting semicolon-separated values.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in tech_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('').apply(\n",
    "                lambda x: [tech.strip() for tech in x.split(';')] if x else []\n",
    "            )\n",
    "    return df\n",
    "\n",
    "# Process technology columns\n",
    "df = process_tech_columns(df)\n",
    "\n",
    "# Check a sample of processed data\n",
    "print(\"Sample of processed technology columns:\")\n",
    "for col in TECH_COLUMNS[:2]:  # Just show first 2 columns as example\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique career paths from the DevType column\n",
    "def extract_career_paths(df, target_col=TARGET_COLUMN):\n",
    "    \"\"\"Extract unique career paths from the target column.\"\"\"\n",
    "    if target_col in df.columns:\n",
    "        # Split semicolon-separated values and flatten the list\n",
    "        all_devtypes = [\n",
    "            devtype.strip() \n",
    "            for devtypes in df[target_col].fillna('').str.split(';') \n",
    "            for devtype in devtypes if devtype.strip()\n",
    "        ]\n",
    "        career_paths = sorted(set(all_devtypes))\n",
    "        print(f\"Found {len(career_paths)} unique career paths\")\n",
    "        return career_paths\n",
    "    return []\n",
    "\n",
    "# Get unique career paths\n",
    "career_paths = extract_career_paths(df)\n",
    "print(\"\\nSample career paths:\")\n",
    "print(career_paths[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process target column (convert from semicolon-separated strings to lists)\n",
    "def process_target_column(df, target_col=TARGET_COLUMN):\n",
    "    \"\"\"Process target column by splitting semicolon-separated values.\"\"\"\n",
    "    df = df.copy()\n",
    "    if target_col in df.columns:\n",
    "        df[target_col] = df[target_col].fillna('').apply(\n",
    "            lambda x: [role.strip() for role in x.split(';')] if x else []\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Process target column\n",
    "df = process_target_column(df)\n",
    "\n",
    "# Check a sample of processed target data\n",
    "print(\"Sample of processed target column:\")\n",
    "print(df[TARGET_COLUMN].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We'll now create features from the technology columns and the YearsCode column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode technology columns\n",
    "def one_hot_encode_tech(df, tech_columns=TECH_COLUMNS):\n",
    "    \"\"\"One-hot encode the technology columns using MultiLabelBinarizer.\"\"\"\n",
    "    # Combine all technology lists for fitting the MultiLabelBinarizer\n",
    "    all_techs = []\n",
    "    for col in tech_columns:\n",
    "        if col in df.columns:\n",
    "            all_techs.extend([tech for techs in df[col] for tech in techs if tech])\n",
    "    \n",
    "    unique_techs = sorted(set(all_techs))\n",
    "    mlb = MultiLabelBinarizer(classes=unique_techs)\n",
    "    mlb.fit([unique_techs])\n",
    "    \n",
    "    tech_features_df = pd.DataFrame()\n",
    "    for col in tech_columns:\n",
    "        if col in df.columns:\n",
    "            # Transform the list column into one-hot encoded columns\n",
    "            col_encoded = mlb.transform(df[col])\n",
    "            col_df = pd.DataFrame(\n",
    "                col_encoded, \n",
    "                columns=[f\"{tech}\" for tech in mlb.classes_],\n",
    "                index=df.index\n",
    "            )\n",
    "            tech_features_df = pd.concat([tech_features_df, col_df], axis=1)\n",
    "    \n",
    "    # Handle duplicate columns by taking the maximum value\n",
    "    tech_features_df = tech_features_df.groupby(tech_features_df.columns, axis=1).max()\n",
    "    \n",
    "    print(f\"Created {tech_features_df.shape[1]} technology features\")\n",
    "    return tech_features_df, mlb\n",
    "\n",
    "# Get one-hot encoded tech features\n",
    "tech_features, mlb = one_hot_encode_tech(df)\n",
    "\n",
    "# Display a sample of the encoded features\n",
    "print(\"\\nSample of one-hot encoded technology features:\")\n",
    "print(tech_features.iloc[:2, :10])  # Show first 2 rows and 10 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "def prepare_features_target(df, tech_features, target_col=TARGET_COLUMN):\n",
    "    \"\"\"Prepare final features (including YearsCode) and target for model training.\"\"\"\n",
    "    # Prepare numerical features (YearsCode)\n",
    "    numerical_features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Include YearsCode as a feature if available\n",
    "    if 'YearsCode' in df.columns:\n",
    "        numerical_features['YearsCode'] = df['YearsCode']\n",
    "        \n",
    "        # Scale the YearsCode column\n",
    "        scaler = StandardScaler()\n",
    "        numerical_features['YearsCode'] = scaler.fit_transform(\n",
    "            numerical_features[['YearsCode']]\n",
    "        )\n",
    "        \n",
    "        print(f\"YearsCode scaled. Mean: {scaler.mean_[0]:.2f}, Std: {scaler.scale_[0]:.2f}\")\n",
    "    else:\n",
    "        print(\"Warning: YearsCode column not found!\")\n",
    "    \n",
    "    # Combine tech features and numerical features\n",
    "    X = pd.concat([tech_features, numerical_features], axis=1)\n",
    "    \n",
    "    # Prepare target variable\n",
    "    y = None\n",
    "    if target_col in df.columns:\n",
    "        y = df[target_col]  # Already processed into lists\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "# Get features and target\n",
    "X, y, scaler = prepare_features_target(df, tech_features)\n",
    "\n",
    "# Display feature information\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(\"\\nFeature columns:\")\n",
    "print(X.columns[-10:])  # Show last 10 columns including YearsCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the target variable\n",
    "def encode_target(y, career_paths=None):\n",
    "    \"\"\"One-hot encode the target variable using MultiLabelBinarizer.\"\"\"\n",
    "    if career_paths is None:\n",
    "        target_mlb = MultiLabelBinarizer()\n",
    "        y_encoded = target_mlb.fit_transform(y)\n",
    "    else:\n",
    "        target_mlb = MultiLabelBinarizer(classes=career_paths)\n",
    "        y_encoded = target_mlb.fit_transform(y)\n",
    "    \n",
    "    print(f\"Target encoded with {y_encoded.shape[1]} classes\")\n",
    "    return y_encoded, target_mlb\n",
    "\n",
    "# Encode the target\n",
    "y_encoded, target_mlb = encode_target(y, career_paths)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "Now we'll implement an efficient deep learning model that specifically handles both technology skills and years of coding experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a hybrid deep learning model that processes tech skills and YearsCode separately\n",
    "def build_hybrid_model(tech_input_dim, num_classes):\n",
    "    \"\"\"Build a hybrid model that processes tech skills and YearsCode separately.\"\"\"\n",
    "    # Tech input branch\n",
    "    tech_input = Input(shape=(tech_input_dim,), name='tech_input')\n",
    "    tech_dense1 = Dense(512, activation='relu')(tech_input)\n",
    "    tech_dropout1 = Dropout(0.3)(tech_dense1)\n",
    "    tech_dense2 = Dense(256, activation='relu')(tech_dropout1)\n",
    "    tech_dropout2 = Dropout(0.3)(tech_dense2)\n",
    "    \n",
    "    # Years code input branch\n",
    "    years_input = Input(shape=(1,), name='years_input')\n",
    "    years_dense = Dense(32, activation='relu')(years_input)\n",
    "    \n",
    "    # Combine branches\n",
    "    combined = Concatenate()([tech_dropout2, years_dense])\n",
    "    combined_dense = Dense(128, activation='relu')(combined)\n",
    "    output = Dense(num_classes, activation='sigmoid')(combined_dense)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=[tech_input, years_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get dimensions for the model\n",
    "tech_input_dim = X_train.shape[1] - 1  # All columns except YearsCode\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# Build the model\n",
    "model = build_hybrid_model(tech_input_dim, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for the hybrid model\n",
    "X_train_tech = X_train.drop(columns=['YearsCode']).values\n",
    "X_train_years = X_train['YearsCode'].values.reshape(-1, 1)\n",
    "X_test_tech = X_test.drop(columns=['YearsCode']).values\n",
    "X_test_years = X_test['YearsCode'].values.reshape(-1, 1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [X_train_tech, X_train_years], y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_data=([X_test_tech, X_test_years], y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, X_test_tech, X_test_years, y_test, target_mlb):\n",
    "    \"\"\"Evaluate the model performance.\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict([X_test_tech, X_test_years])\n",
    "    \n",
    "    # Convert probabilities to binary predictions (threshold 0.5)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Micro-F1: {micro_f1:.4f}\")\n",
    "    print(f\"Macro-F1: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Show the top classes with highest positive counts\n",
    "    class_counts = y_pred.sum(axis=0)\n",
    "    class_names = target_mlb.classes_\n",
    "    \n",
    "    top_indices = np.argsort(-class_counts)[:10]  # Top 10 classes\n",
    "    print(\"\\nTop 10 predicted career paths:\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"{class_names[idx]}: {class_counts[idx]} predictions\")\n",
    "    \n",
    "    return y_pred, y_pred_proba\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred, y_pred_proba = evaluate_model(model, X_test_tech, X_test_years, y_test, target_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze years of experience for different career paths\n",
    "def analyze_years_by_career_path(df, scaler):\n",
    "    \"\"\"Analyze the relationship between YearsCode and career paths.\"\"\"\n",
    "    # Create a dataframe with DevType and YearsCode\n",
    "    years_by_role = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        roles = row[TARGET_COLUMN]\n",
    "        years = row['YearsCode']\n",
    "        \n",
    "        for role in roles:\n",
    "            years_by_role.append({\n",
    "                'Role': role,\n",
    "                'YearsCode': years\n",
    "            })\n",
    "    \n",
    "    years_df = pd.DataFrame(years_by_role)\n",
    "    \n",
    "    # Group by role and calculate mean years\n",
    "    role_years = years_df.groupby('Role')['YearsCode'].agg(['mean', 'median', 'count']).reset_index()\n",
    "    role_years = role_years.sort_values('mean', ascending=False)\n",
    "    \n",
    "    # Only keep roles with at least 50 instances to ensure statistical significance\n",
    "    role_years = role_years[role_years['count'] >= 50]\n",
    "    \n",
    "    print(\"Average years of coding experience by career path:\")\n",
    "    print(role_years.head(10))\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='mean', y='Role', data=role_years.head(15), palette='viridis')\n",
    "    plt.title('Average Years of Coding Experience by Career Path')\n",
    "    plt.xlabel('Years of Coding Experience')\n",
    "    plt.ylabel('Career Path')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return role_years\n",
    "\n",
    "# Analyze years by career path\n",
    "role_years = analyze_years_by_career_path(df, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Related Components\n",
    "\n",
    "Now we'll save the model and related components for use in the FastAPI backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and related components\n",
    "def save_model(model, mlb, target_mlb, scaler, save_dir=\"../models\"):\n",
    "    \"\"\"Save the model and related components.\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(save_dir, \"career_path_model\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Save components needed for preprocessing and postprocessing\n",
    "    components = {\n",
    "        'tech_mlb': mlb,\n",
    "        'target_mlb': target_mlb,\n",
    "        'scaler': scaler,\n",
    "        'career_paths': list(target_mlb.classes_)\n",
    "    }\n",
    "    \n",
    "    components_path = os.path.join(save_dir, \"model_components.joblib\")\n",
    "    joblib.dump(components, components_path)\n",
    "    print(f\"Model components saved to {components_path}\")\n",
    "    \n",
    "# Save the model and components\n",
    "save_model(model, mlb, target_mlb, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loading and prediction\n",
    "def test_model_loading(model_dir=\"../models\", sample_text=None):\n",
    "    \"\"\"Test loading the model and making predictions.\"\"\"\n",
    "    # Load model\n",
    "    model_path = os.path.join(model_dir, \"career_path_model\")\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load components\n",
    "    components_path = os.path.join(model_dir, \"model_components.joblib\")\n",
    "    components = joblib.load(components_path)\n",
    "    \n",
    "    tech_mlb = components['tech_mlb']\n",
    "    target_mlb = components['target_mlb']\n",
    "    scaler = components['scaler']\n",
    "    career_paths = components['career_paths']\n",
    "    \n",
    "    print(f\"Model loaded successfully. Can predict {len(career_paths)} career paths.\")\n",
    "    \n",
    "    # If a sample text is provided, demonstrate prediction\n",
    "    if sample_text:\n",
    "        from src.features import ConceptMatcher\n",
    "        \n",
    "        # Initialize technology matcher\n",
    "        matcher = ConceptMatcher()\n",
    "        matcher.load_concepts()\n",
    "        matcher.generate_concept_embeddings()\n",
    "        \n",
    "        # Extract technologies from text\n",
    "        matcher.prepare_candidate_phrases(sample_text)\n",
    "        matcher.vectorized_match_candidates()\n",
    "        matcher.global_filtering()\n",
    "        \n",
    "        # Get recognized technologies\n",
    "        recognized_techs = matcher.get_recognized_technologies()\n",
    "        print(f\"\\nRecognized technologies: {recognized_techs}\")\n",
    "        \n",
    "        # Extract years of experience using simple pattern matching\n",
    "        import re\n",
    "        years_pattern = r'(\\d+)\\s+years?\\s+(?:of\\s+)?(?:coding|programming|experience|work)'\n",
    "        years_matches = re.findall(years_pattern, sample_text, re.IGNORECASE)\n",
    "        years_code = float(years_matches[0]) if years_matches else 5.0  # Default to 5 years if not found\n",
    "        print(f\"Extracted years of experience: {years_code}\")\n",
    "        \n",
    "        # Prepare features\n",
    "        tech_features = pd.DataFrame(np.zeros((1, len(tech_mlb.classes_))), columns=tech_mlb.classes_)\n",
    "        for tech in recognized_techs:\n",
    "            if tech in tech_features.columns:\n",
    "                tech_features[tech] = 1\n",
    "        \n",
    "        # Scale YearsCode\n",
    "        years_code_scaled = scaler.transform([[years_code]])[0][0]\n",
    "        \n",
    "        # Prepare inputs for the model\n",
    "        tech_input = tech_features.values\n",
    "        years_input = np.array([[years_code_scaled]])\n",
    "        \n",
    "        # Make prediction\n",
    "        y_pred_proba = loaded_model.predict([tech_input, years_input])[0]\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top_indices = np.argsort(-y_pred_proba)[:5]\n",
    "        print(\"\\nTop 5 predicted career paths:\")\n",
    "        for idx in top_indices:\n",
    "            role = career_paths[idx]\n",
    "            prob = y_pred_proba[idx]\n",
    "            print(f\"{role}: {prob:.4f} probability\")\n",
    "    \n",
    "    return loaded_model, components\n",
    "\n",
    "# Test model loading with a sample text\n",
    "sample_text = \"\"\"\n",
    "I have 8 years of experience as a software developer, primarily working with Python, JavaScript, and SQL. \n",
    "I've built several web applications using React and Node.js, and I've worked extensively with AWS cloud services. \n",
    "I also have experience with Docker, Kubernetes, and CI/CD pipelines.\n",
    "\"\"\"\n",
    "\n",
    "loaded_model, components = test_model_loading(sample_text=sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAPI Implementation\n",
    "\n",
    "Below is the structure of the FastAPI implementation that will be created in the appropriate Python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code example for the FastAPI implementation\n",
    "# The actual implementation will be in src/services/api.py\n",
    "\n",
    "'''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "import uvicorn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "\n",
    "from src.features import ConceptMatcher\n",
    "\n",
    "# Load model and components\n",
    "MODEL_DIR = \"../models\"\n",
    "model = tf.keras.models.load_model(os.path.join(MODEL_DIR, \"career_path_model\"))\n",
    "components = joblib.load(os.path.join(MODEL_DIR, \"model_components.joblib\"))\n",
    "tech_mlb = components[\"tech_mlb\"]\n",
    "target_mlb = components[\"target_mlb\"]\n",
    "scaler = components[\"scaler\"]\n",
    "career_paths = components[\"career_paths\"]\n",
    "\n",
    "# Initialize ConceptMatcher\n",
    "matcher = ConceptMatcher()\n",
    "matcher.load_concepts()\n",
    "matcher.generate_concept_embeddings(load_if_exists=True)\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"CareerConnect API\",\n",
    "    description=\"API for career path recommendations based on technical skills and experience\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Input models\n",
    "class TextInput(BaseModel):\n",
    "    \"\"\"Input model for text-based predictions.\"\"\"\n",
    "    text: str = Field(..., \n",
    "        description=\"Text describing skills, technologies, and years of coding experience\",\n",
    "        example=\"I have 5 years of experience with Python, JavaScript, and React. I've also worked with AWS and Docker.\")\n",
    "    \n",
    "class FeatureInput(BaseModel):\n",
    "    \"\"\"Input model for feature-based predictions.\"\"\"\n",
    "    features: Dict[str, float] = Field(..., \n",
    "        description=\"Dictionary of technology skills as keys and 1.0 as values to indicate presence\")\n",
    "    years_code: float = Field(..., \n",
    "        description=\"Years of coding experience\", \n",
    "        example=5.0)\n",
    "\n",
    "# Output models\n",
    "class CareerPrediction(BaseModel):\n",
    "    \"\"\"Model for career path prediction result.\"\"\"\n",
    "    career_path: str\n",
    "    probability: float\n",
    "    \n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Response model for prediction endpoints.\"\"\"\n",
    "    predictions: List[CareerPrediction]\n",
    "    recognized_techs: Optional[List[str]] = None\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint.\"\"\"\n",
    "    return {\"message\": \"Welcome to CareerConnect API\"}\n",
    "\n",
    "@app.post(\"/predict/text\", response_model=PredictionResponse)\n",
    "async def predict_from_text(input_data: TextInput):\n",
    "    \"\"\"Predict career paths from text description of skills and experience.\"\"\"\n",
    "    try:\n",
    "        # Extract technologies from text\n",
    "        matcher.prepare_candidate_phrases(input_data.text)\n",
    "        matcher.vectorized_match_candidates()\n",
    "        matcher.global_filtering()\n",
    "        recognized_techs = matcher.get_recognized_technologies()\n",
    "        \n",
    "        # Extract years of experience using pattern matching\n",
    "        years_pattern = r\"(\\d+)\\s+years?\\s+(?:of\\s+)?(?:coding|programming|experience|work)\"\n",
    "        years_matches = re.findall(years_pattern, input_data.text, re.IGNORECASE)\n",
    "        years_code = float(years_matches[0]) if years_matches else 5.0  # Default to 5 years if not found\n",
    "        \n",
    "        # Prepare features\n",
    "        tech_features = pd.DataFrame(np.zeros((1, len(tech_mlb.classes_))), columns=tech_mlb.classes_)\n",
    "        for tech in recognized_techs:\n",
    "            if tech in tech_features.columns:\n",
    "                tech_features[tech] = 1\n",
    "        \n",
    "        # Scale YearsCode\n",
    "        years_code_scaled = scaler.transform([[years_code]])[0][0]\n",
    "        \n",
    "        # Prepare inputs for the model\n",
    "        tech_input = tech_features.values\n",
    "        years_input = np.array([[years_code_scaled]])\n",
    "        \n",
    "        # Make prediction\n",
    "        y_pred_proba = model.predict([tech_input, years_input])[0]\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top_indices = np.argsort(-y_pred_proba)[:5]\n",
    "        predictions = []\n",
    "        for idx in top_indices:\n",
    "            prob = float(y_pred_proba[idx])\n",
    "            if prob < 0.1:  # Skip predictions with very low probability\n",
    "                continue\n",
    "            predictions.append({\n",
    "                \"career_path\": career_paths[idx],\n",
    "                \"probability\": prob\n",
    "            })\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            \"predictions\": predictions,\n",
    "            \"recognized_techs\": recognized_techs\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Prediction error: {str(e)}\")\n",
    "\n",
    "@app.post(\"/predict/features\", response_model=PredictionResponse)\n",
    "async def predict_from_features(input_data: FeatureInput):\n",
    "    \"\"\"Predict career paths from explicit feature dictionary.\"\"\"\n",
    "    try:\n",
    "        # Prepare features\n",
    "        tech_features = pd.DataFrame(np.zeros((1, len(tech_mlb.classes_))), columns=tech_mlb.classes_)\n",
    "        for tech, value in input_data.features.items():\n",
    "            if tech in tech_features.columns:\n",
    "                tech_features[tech] = value\n",
    "        \n",
    "        # Scale YearsCode\n",
    "        years_code_scaled = scaler.transform([[input_data.years_code]])[0][0]\n",
    "        \n",
    "        # Prepare inputs for the model\n",
    "        tech_input = tech_features.values\n",
    "        years_input = np.array([[years_code_scaled]])\n",
    "        \n",
    "        # Make prediction\n",
    "        y_pred_proba = model.predict([tech_input, years_input])[0]\n",
    "        \n",
    "        # Get top 5 predictions\n",
    "        top_indices = np.argsort(-y_pred_proba)[:5]\n",
    "        predictions = []\n",
    "        for idx in top_indices:\n",
    "            prob = float(y_pred_proba[idx])\n",
    "            if prob < 0.1:  # Skip predictions with very low probability\n",
    "                continue\n",
    "            predictions.append({\n",
    "                \"career_path\": career_paths[idx],\n",
    "                \"probability\": prob\n",
    "            })\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            \"predictions\": predictions,\n",
    "            \"recognized_techs\": list(input_data.features.keys())\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Prediction error: {str(e)}\")\n",
    "\n",
    "@app.get(\"/career-paths\", response_model=List[str])\n",
    "async def get_career_paths():\n",
    "    \"\"\"Get all possible career paths from the model.\"\"\"\n",
    "    return career_paths\n",
    "\n",
    "@app.get(\"/technologies\", response_model=List[str])\n",
    "async def get_technologies():\n",
    "    \"\"\"Get all technologies that can be used for prediction.\"\"\"\n",
    "    return list(tech_mlb.classes_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\"api:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the implementation of a career path prediction model that uses technology skills and years of coding experience (`YearsCode`) as features. The model specifically processes the YearsCode column separately to recognize its importance for roles like Engineering Manager and Project Manager that require coding experience.\n",
    "\n",
    "The implementation includes:\n",
    "\n",
    "1. **Data preprocessing** with proper handling of the YearsCode numerical column\n",
    "2. **Feature engineering** that combines technology skills and years of experience\n",
    "3. **Efficient deep learning model** with a hybrid architecture that processes tech skills and YearsCode in separate branches\n",
    "4. **FastAPI backend** design for serving the model through REST API endpoints\n",
    "\n",
    "The model demonstrates how years of coding experience is a critical factor in determining appropriate career paths, especially for management roles that require technical expertise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
