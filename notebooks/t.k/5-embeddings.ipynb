{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fe12ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\deep_learning\\Lib\\site-packages\\h5py\\__init__.py:36: UserWarning: h5py is running against HDF5 1.14.3 when it was built against 1.14.2, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda3\\envs\\deep_learning\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loaded technologies data with 238 entries\n",
      "Total StackOverflow Concepts (including abbreviations): 321\n",
      "Loaded existing concept embeddings from stack_concept_embeddings_all-mpnet-base-v2.npy\n",
      "Graph contains 313 nodes and 34 edges.\n",
      "Total candidate phrases generated: 98\n",
      "Total recognized candidate matches: 37\n",
      "Global filtering completed.\n",
      "\n",
      "Globally Filtered Recognized Concepts using n‑gram detection (from StackOverflow data):\n",
      "============================================================\n",
      "Concept: TensorFlow (Abbreviation)\n",
      "    Detected 1-gram: 'tf' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: TypeScript (Abbreviation)\n",
      "    Detected 1-gram: 'ts' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: Objective-C (Abbreviation)\n",
      "    Detected 1-gram: 'objc' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: React (Technology)\n",
      "    Detected 1-gram: 'react' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: Vue.js (Abbreviation)\n",
      "    Detected 1-gram: 'vue' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: Amazon Web Services (AWS) (Abbreviation)\n",
      "    Detected 1-gram: 'aws' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: Python (Abbreviation)\n",
      "    Detected 1-gram: 'py' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: Node.js (Abbreviation)\n",
      "    Detected 1-gram: 'node' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: Microsoft SQL Server (Abbreviation)\n",
      "    Detected 1-gram: 'mssql' with similarity 1.00\n",
      "------------------------------------------------------------\n",
      "Concept: Scikit-Learn (Abbreviation)\n",
      "    Detected 1-gram: 'sklearn' with similarity 1.00\n",
      "    Detected 2-gram: 'machine learning' with similarity 0.54\n",
      "------------------------------------------------------------\n",
      "Concept: Google Cloud (Technology)\n",
      "    Detected 2-gram: 'cloud computing' with similarity 0.66\n",
      "------------------------------------------------------------\n",
      "Concept: Microsoft Azure (Technology)\n",
      "    Detected 3-gram: 'cloud computing platforms' with similarity 0.64\n",
      "------------------------------------------------------------\n",
      "Concept: PlatformIO (Technology)\n",
      "    Detected 1-gram: 'platforms' with similarity 0.59\n",
      "------------------------------------------------------------\n",
      "Concept: Solidity (Technology)\n",
      "    Detected 1-gram: 'computing' with similarity 0.56\n",
      "------------------------------------------------------------\n",
      "Concept: Cloudflare (Technology)\n",
      "    Detected 1-gram: 'cloud' with similarity 0.55\n",
      "------------------------------------------------------------\n",
      "Concept: VMware (Technology)\n",
      "    Detected 2-gram: 'computing platforms' with similarity 0.54\n",
      "------------------------------------------------------------\n",
      "Concept: Render (Technology)\n",
      "    Detected 1-gram: 'graphical' with similarity 0.52\n",
      "------------------------------------------------------------\n",
      "Concept: Databricks (Technology)\n",
      "    Detected 1-gram: 'data' with similarity 0.50\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure required NLTK resources are available.\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "class ConceptMatcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path=\"../data/processed/technologies_with_abbreviations.csv\",  # Updated path to new CSV with abbreviations\n",
    "        columns=None,\n",
    "        model_name=\"all-mpnet-base-v2\",\n",
    "        similarity_threshold_graph=0.7,\n",
    "        ngram_threshold=0.5,\n",
    "        filter_similarity_threshold=0.85\n",
    "    ):\n",
    "        if columns is None:\n",
    "            columns = [\n",
    "                \"LanguageHaveWorkedWith\", \"DatabaseHaveWorkedWith\", \"PlatformHaveWorkedWith\",\n",
    "                \"WebframeHaveWorkedWith\", \"EmbeddedHaveWorkedWith\", \"MiscTechHaveWorkedWith\",\n",
    "                \"ToolsTechHaveWorkedWith\"\n",
    "            ]\n",
    "        self.csv_path = csv_path\n",
    "        self.columns = columns\n",
    "        self.model_name = model_name\n",
    "        self.similarity_threshold_graph = similarity_threshold_graph\n",
    "        self.ngram_threshold = ngram_threshold\n",
    "        self.filter_similarity_threshold = filter_similarity_threshold\n",
    "\n",
    "        # Initialize NLTK stop words and custom filter words.\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.custom_filter_words = {\n",
    "            'additionally', 'also', 'furthermore', 'moreover', 'including', 'like', 'career', 'etc'\n",
    "        }\n",
    "\n",
    "        # Initialize the SentenceTransformer model.\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "        # Placeholders for later processing.\n",
    "        self.tech_data = None          # DataFrame to store technology names and abbreviations\n",
    "        self.stack_concepts = []       # List of concept dictionaries.\n",
    "        self.concept_embeddings = None # Numpy array of concept embeddings.\n",
    "        self.candidate_phrases = []    # Candidate n‑gram phrases from input text.\n",
    "        self.candidate_embeddings = None  # Numpy array of candidate embeddings.\n",
    "        self.recognized_candidates_ngram = []  # Matched candidates with similarity scores.\n",
    "        self.filtered_by_concept = {}     # Final grouped output after global filtering.\n",
    "        self.graph = None                 # Optional similarity graph.\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Lowercase the text and remove punctuation except for hyphens and parentheses.\n",
    "        \"\"\"\n",
    "        # Preserve '-' and parentheses by removing other punctuation.\n",
    "        punctuation_to_remove = \"\".join(ch for ch in string.punctuation if ch not in \"-()\")\n",
    "        text = text.lower().translate(str.maketrans(\"\", \"\", punctuation_to_remove))\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def is_meaningful(self, phrase):\n",
    "        tokens = [t.lower() for t in word_tokenize(phrase) if t.isalpha()]\n",
    "        if not tokens:\n",
    "            return False\n",
    "        if any(token in self.custom_filter_words for token in tokens):\n",
    "            return False\n",
    "        if len(tokens) == 1 and tokens[0] in self.stop_words:\n",
    "            return False\n",
    "        if tokens and sum(1 for t in tokens if t in self.stop_words) / len(tokens) > 0.5:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def load_concepts(self):\n",
    "        # Load the technology data with abbreviations\n",
    "        self.tech_data = pd.read_csv(self.csv_path)\n",
    "        print(f\"Loaded technologies data with {len(self.tech_data)} entries\")\n",
    "        \n",
    "        # Create concepts from both technology names and their abbreviations\n",
    "        self.stack_concepts = []\n",
    "        \n",
    "        # Add technology names first\n",
    "        for _, row in self.tech_data.iterrows():\n",
    "            tech_name = row['Technology']\n",
    "            self.stack_concepts.append({\"name\": tech_name, \"type\": \"Technology\", \"original\": tech_name})\n",
    "        \n",
    "        # Add abbreviations if they exist\n",
    "        for _, row in self.tech_data.iterrows():\n",
    "            if pd.notna(row['abrv']) and row['abrv'].strip():  # Check if abbreviation exists and is not empty\n",
    "                tech_name = row['Technology']\n",
    "                abbr = row['abrv']\n",
    "                self.stack_concepts.append({\"name\": abbr, \"type\": \"Abbreviation\", \"original\": tech_name})\n",
    "        \n",
    "        print(f\"Total StackOverflow Concepts (including abbreviations): {len(self.stack_concepts)}\")\n",
    "\n",
    "    def generate_concept_embeddings(self, save_embeddings=True, load_if_exists=True):\n",
    "        \"\"\"\n",
    "        Generate embeddings for all technology concepts or load existing ones if available.\n",
    "        \n",
    "        Parameters:\n",
    "        - save_embeddings: Whether to save newly generated embeddings\n",
    "        - load_if_exists: Whether to try loading existing embeddings first\n",
    "        \"\"\"\n",
    "        filename = f\"stack_concept_embeddings_{self.model_name.replace('/', '_')}.npy\"\n",
    "        \n",
    "        # Try to load existing embeddings if requested\n",
    "        if load_if_exists:\n",
    "            try:\n",
    "                self.concept_embeddings = np.load(filename)\n",
    "                print(f\"Loaded existing concept embeddings from {filename}\")\n",
    "                return\n",
    "            except FileNotFoundError:\n",
    "                print(f\"No existing embeddings found at {filename}, generating new ones...\")\n",
    "        \n",
    "        # Generate new embeddings\n",
    "        concept_texts = [concept[\"name\"] for concept in self.stack_concepts]\n",
    "        self.concept_embeddings = self.model.encode(concept_texts, convert_to_numpy=True)\n",
    "        \n",
    "        if save_embeddings:\n",
    "            np.save(filename, self.concept_embeddings)\n",
    "            print(f\"Concept embeddings saved to {filename}\")\n",
    "\n",
    "    def build_similarity_graph(self):\n",
    "        self.graph = nx.Graph()\n",
    "        concept_texts = [concept[\"name\"] for concept in self.stack_concepts]\n",
    "        for concept in self.stack_concepts:\n",
    "            self.graph.add_node(concept[\"name\"], category=concept[\"type\"], original=concept.get(\"original\", concept[\"name\"]))\n",
    "        sim_matrix = cosine_similarity(self.concept_embeddings)\n",
    "        for i in range(len(concept_texts)):\n",
    "            for j in range(i + 1, len(concept_texts)):\n",
    "                if sim_matrix[i][j] >= self.similarity_threshold_graph:\n",
    "                    self.graph.add_edge(concept_texts[i], concept_texts[j], weight=sim_matrix[i][j])\n",
    "        print(f\"Graph contains {len(self.graph.nodes)} nodes and {len(self.graph.edges)} edges.\")\n",
    "\n",
    "    def prepare_candidate_phrases(self, long_text):\n",
    "        cleaned_full_text = self.clean_text(long_text)\n",
    "        tokens_clean = word_tokenize(cleaned_full_text)\n",
    "        candidate_phrases = []\n",
    "        for n in [3, 2, 1]:\n",
    "            for gram in ngrams(tokens_clean, n):\n",
    "                phrase = \" \".join(gram)\n",
    "                if phrase.strip() and self.is_meaningful(phrase):\n",
    "                    candidate_phrases.append(phrase)\n",
    "        self.candidate_phrases = list(set(candidate_phrases))\n",
    "        print(f\"Total candidate phrases generated: {len(self.candidate_phrases)}\")\n",
    "\n",
    "    def vectorized_match_candidates(self):\n",
    "        self.candidate_embeddings = self.model.encode(self.candidate_phrases, convert_to_numpy=True)\n",
    "        similarity_matrix = cosine_similarity(self.candidate_embeddings, self.concept_embeddings)\n",
    "        max_similarities = similarity_matrix.max(axis=1)\n",
    "        max_indices = similarity_matrix.argmax(axis=1)\n",
    "        valid_indices = np.where(max_similarities >= self.ngram_threshold)[0]\n",
    "        self.recognized_candidates_ngram = []\n",
    "        for idx in valid_indices:\n",
    "            max_sim = max_similarities[idx]\n",
    "            max_idx = max_indices[idx]\n",
    "            concept_name = self.stack_concepts[max_idx][\"name\"]\n",
    "            concept_type = self.stack_concepts[max_idx][\"type\"]\n",
    "            original_name = self.stack_concepts[max_idx].get(\"original\", concept_name)\n",
    "            phrase = self.candidate_phrases[idx]\n",
    "            n_val = len(phrase.split())\n",
    "            tokens_phrase = phrase.split()\n",
    "            self.recognized_candidates_ngram.append(\n",
    "                (original_name, concept_type, phrase, max_sim, n_val, tokens_phrase)\n",
    "            )\n",
    "        print(f\"Total recognized candidate matches: {len(self.recognized_candidates_ngram)}\")\n",
    "\n",
    "    def global_filtering(self):\n",
    "        recognized = sorted(self.recognized_candidates_ngram, key=lambda x: x[3], reverse=True)\n",
    "        global_used_words = set()\n",
    "        filtered_candidates = []\n",
    "        for candidate in recognized:\n",
    "            concept_name, concept_type, phrase, score, n_val, tokens_phrase = candidate\n",
    "            if any(token in global_used_words for token in tokens_phrase):\n",
    "                continue\n",
    "            filtered_candidates.append(candidate)\n",
    "            if score > self.filter_similarity_threshold:\n",
    "                global_used_words.update(tokens_phrase)\n",
    "        self.filtered_by_concept = {}\n",
    "        for concept_name, concept_type, phrase, score, n_val, tokens_phrase in filtered_candidates:\n",
    "            self.filtered_by_concept.setdefault(concept_name, {\"type\": concept_type, \"phrases\": []})\n",
    "            self.filtered_by_concept[concept_name][\"phrases\"].append((phrase, score, n_val, tokens_phrase))\n",
    "        print(\"Global filtering completed.\")\n",
    "\n",
    "    def print_results(self):\n",
    "        print(\"\\nGlobally Filtered Recognized Concepts using n‑gram detection (from StackOverflow data):\")\n",
    "        print(\"=\" * 60)\n",
    "        for concept, info in self.filtered_by_concept.items():\n",
    "            concept_type = info[\"type\"]\n",
    "            print(f\"Concept: {concept} ({concept_type})\")\n",
    "            for phrase, score, n_val, tokens_phrase in sorted(info[\"phrases\"], key=lambda x: x[1], reverse=True):\n",
    "                print(f\"    Detected {n_val}-gram: '{phrase}' with similarity {score:.2f}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    def get_recognized_technologies(self):\n",
    "        \"\"\"Get a simple list of recognized technologies.\"\"\"\n",
    "        return list(self.filtered_by_concept.keys())\n",
    "\n",
    "model_names = [\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/msmarco-distilbert-base-v4\",\n",
    "    \"all-mpnet-base-v2\"\n",
    "]\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    matcher = ConceptMatcher(\n",
    "        csv_path=\"../data/processed/technologies_with_abbreviations.csv\",\n",
    "        model_name=\"sentence-transformers/msmarco-distilbert-base-v4\",\n",
    "        similarity_threshold_graph=0.7,\n",
    "        ngram_threshold=0.5,\n",
    "        filter_similarity_threshold=0.85\n",
    "    )\n",
    "    matcher.load_concepts()\n",
    "    matcher.generate_concept_embeddings(save_embeddings=True, load_if_exists=True)\n",
    "    matcher.build_similarity_graph()\n",
    "    \n",
    "    # Sample text now includes abbreviations that should be detected\n",
    "    sample_text = \"\"\"\n",
    "    I have extensive experience in data analysis and have worked with a variety of technologies including MSSQL,\n",
    "    Py, ObjC, cloud computing platforms like AWS, and I am proficient with machine learning techniques using TF and Sklearn. My background also includes\n",
    "    developing user interfaces with React and Vue. Additionally, I have hands-on experience with business intelligence and\n",
    "    graphical user interface design using Node and TS.\n",
    "    \"\"\"\n",
    "    matcher.prepare_candidate_phrases(sample_text)\n",
    "    matcher.vectorized_match_candidates()\n",
    "    matcher.global_filtering()\n",
    "    matcher.print_results()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
