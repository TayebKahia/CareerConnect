{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph contains 1114 nodes and 450 edges.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Load and extract O*NET concepts as before\n",
    "with open(\"abbr_cleaned_IT_data_from_onet.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    onet_data = json.load(f)\n",
    "\n",
    "onet_skill_titles = set()\n",
    "onet_tech_names = set()\n",
    "\n",
    "for job in onet_data:\n",
    "    for tech_skill in job.get(\"technology_skills\", []):\n",
    "        if \"skill_title\" in tech_skill:\n",
    "            onet_skill_titles.add(tech_skill[\"skill_title\"])\n",
    "        for tech_item in tech_skill.get(\"technologies\", []):\n",
    "            onet_tech_names.add(tech_item[\"name\"])\n",
    "\n",
    "# Combine into a list of dictionaries\n",
    "onet_concepts = (\n",
    "    [{\"name\": title, \"type\": \"skill_title\"} for title in onet_skill_titles] +\n",
    "    [{\"name\": tech, \"type\": \"technology_name\"} for tech in onet_tech_names]\n",
    ")\n",
    "\n",
    "# Process each concept to separate the main text and the abbreviation\n",
    "processed_concepts = []\n",
    "for concept in onet_concepts:\n",
    "    full_text = concept[\"name\"]\n",
    "    # Get the main part (everything before the first parenthesis)\n",
    "    main_text = re.sub(r'\\s*\\(.*', '', full_text).strip()\n",
    "    # Extract abbreviation if available\n",
    "    abbr_match = re.search(r'\\((.*?)\\)', full_text)\n",
    "    abbr_text = abbr_match.group(1).strip() if abbr_match else \"\"\n",
    "    processed_concepts.append({\n",
    "        \"name\": full_text,\n",
    "        \"type\": concept[\"type\"],\n",
    "        \"main\": main_text,\n",
    "        \"abbr\": abbr_text\n",
    "    })\n",
    "\n",
    "# Initialize the model\n",
    "model_name = \"sentence-transformers/msmarco-distilbert-base-v4\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Create lists for the main texts and abbreviation texts\n",
    "main_texts = [item[\"main\"] for item in processed_concepts]\n",
    "abbr_texts = [item[\"abbr\"] for item in processed_concepts]\n",
    "\n",
    "# Generate embeddings for both parts\n",
    "main_embeddings = model.encode(main_texts, convert_to_numpy=True)\n",
    "abbr_embeddings = model.encode(abbr_texts, convert_to_numpy=True)\n",
    "\n",
    "# (Optional) Save dual embeddings along with the processed concepts for later use.\n",
    "np.savez(f\"onet_concept_embeddings_{model_name.replace('/', '_')}.npz\",\n",
    "         main=main_embeddings, abbr=abbr_embeddings)\n",
    "with open(f\"processed_onet_concepts_{model_name.replace('/', '_')}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_concepts, f, indent=4)\n",
    "\n",
    "# (Optional) Build a similarity graph using the main embeddings\n",
    "G = nx.Graph()\n",
    "for concept in processed_concepts:\n",
    "    G.add_node(concept[\"name\"], category=concept[\"type\"])\n",
    "\n",
    "similarity_matrix = cosine_similarity(main_embeddings)\n",
    "SIMILARITY_THRESHOLD = 0.7  # For graph creation only\n",
    "for i in range(len(main_texts)):\n",
    "    for j in range(i + 1, len(main_texts)):\n",
    "        if similarity_matrix[i][j] >= SIMILARITY_THRESHOLD:\n",
    "            G.add_edge(main_texts[i], main_texts[j],\n",
    "                       weight=similarity_matrix[i][j])\n",
    "\n",
    "print(f\"Graph contains {len(G.nodes)} nodes and {len(G.edges)} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zinou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zinou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Globally Filtered Recognized Concepts using n窶組ram detection:\n",
      "============================================================\n",
      "Concept: Microsoft Azure software (Azure) (technology_name)\n",
      "    Detected 1-gram: 'azure' with similarity 1.00 (matched with abbr text)\n",
      "------------------------------------------------------------\n",
      "Concept: MySQL (technology_name)\n",
      "    Detected 1-gram: 'mysql' with similarity 1.00 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: MongoDB (technology_name)\n",
      "    Detected 1-gram: 'mongodb' with similarity 1.00 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Python (technology_name)\n",
      "    Detected 1-gram: 'python' with similarity 1.00 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Amazon Web Services software (AWS) (technology_name)\n",
      "    Detected 1-gram: 'aws' with similarity 1.00 (matched with abbr text)\n",
      "------------------------------------------------------------\n",
      "Concept: RESTful (REST API) (technology_name)\n",
      "    Detected 2-gram: 'rest apis' with similarity 0.98 (matched with abbr text)\n",
      "------------------------------------------------------------\n",
      "Concept: Node.js (technology_name)\n",
      "    Detected 1-gram: 'nodejs' with similarity 0.69 (matched with main text)\n",
      "    Detected 2-gram: 'nodejs built' with similarity 0.57 (matched with main text)\n",
      "    Detected 3-gram: 'nodejs built robust' with similarity 0.51 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Amazon Web Services CloudFormation (CloudFormation) (technology_name)\n",
      "    Detected 2-gram: 'cloud services' with similarity 0.68 (matched with main text)\n",
      "    Detected 1-gram: 'cloud' with similarity 0.63 (matched with abbr text)\n",
      "    Detected 3-gram: 'extends cloud services' with similarity 0.53 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Database management systems (DBMS) (technology_name)\n",
      "    Detected 1-gram: 'databases' with similarity 0.66 (matched with main text)\n",
      "    Detected 2-gram: 'various databases' with similarity 0.61 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Web application software (Web App) (technology_name)\n",
      "    Detected 1-gram: 'applications' with similarity 0.63 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Development environment software (Dev Env SW) (skill_title)\n",
      "    Detected 2-gram: 'development using' with similarity 0.58 (matched with main text)\n",
      "    Detected 1-gram: 'development' with similarity 0.56 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Scala (technology_name)\n",
      "    Detected 1-gram: 'scalable' with similarity 0.57 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Database management software (DB Mgmt SW) (technology_name)\n",
      "    Detected 3-gram: 'worked various databases' with similarity 0.54 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: ServiceNow (technology_name)\n",
      "    Detected 1-gram: 'services' with similarity 0.54 (matched with main text)\n",
      "------------------------------------------------------------\n",
      "Concept: Web platform development software (Web Dev SW) (skill_title)\n",
      "    Detected 2-gram: 'backend development' with similarity 0.54 (matched with main text)\n",
      "    Detected 3-gram: 'backend development using' with similarity 0.52 (matched with main text)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words and custom filter words.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_filter_words = {'additionally', 'also', 'furthermore',\n",
    "                       'moreover', 'including', 'like', 'career', 'etc'}\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase and remove punctuation.\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([token for token in tokens if token not in stop_words])\n",
    "\n",
    "\n",
    "def is_meaningful(phrase):\n",
    "    tokens = [t.lower() for t in word_tokenize(phrase) if t.isalpha()]\n",
    "    if not tokens:\n",
    "        return False\n",
    "    if any(token in custom_filter_words for token in tokens):\n",
    "        return False\n",
    "    if len(tokens) == 1 and tokens[0] in stop_words:\n",
    "        return False\n",
    "    if sum(1 for t in tokens if t in stop_words)/len(tokens) > 0.5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Process Resume Text\n",
    "# ---------------------------\n",
    "# long_text = \"\"\"\n",
    "# I have extensive experience in data analysis and have worked with a variety of technologies including Microsoft SQL Server,\n",
    "# Python, cloud computing platforms like AWS, and I am proficient with machine learning techniques. My background also includes\n",
    "# developing user interfaces with modern tools. Additionally, I have hands-on experience with business intelligence and\n",
    "# graphical user interface design.\n",
    "# \"\"\"\n",
    "long_text = \"\"\"Throughout my career, I have developed expertise in backend development using Python and Node.js. I have built robust REST APIs and worked with various databases including MySQL and MongoDB. My experience also extends to cloud services like AWS and Azure,\n",
    "enabling me to deploy scalable applications.\n",
    "\"\"\"\n",
    "cleaned_full_text = clean_text(long_text)\n",
    "tokens_clean = word_tokenize(cleaned_full_text)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Generate Candidate Phrases using n窶組rams\n",
    "# ---------------------------\n",
    "candidate_phrases = []\n",
    "for n in [3, 2, 1]:\n",
    "    for gram in ngrams(tokens_clean, n):\n",
    "        phrase = \" \".join(gram)\n",
    "        if phrase.strip() and is_meaningful(phrase):\n",
    "            candidate_phrases.append(phrase)\n",
    "candidate_phrases = list(set(candidate_phrases))  # Remove duplicates\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Improved Matching Against Concepts\n",
    "# ---------------------------\n",
    "# Load the pre-generated dual embeddings and processed concepts.\n",
    "model_name = \"sentence-transformers/msmarco-distilbert-base-v4\"\n",
    "data = np.load(f\"onet_concept_embeddings_{model_name.replace('/', '_')}.npz\")\n",
    "main_embeddings = data['main']\n",
    "abbr_embeddings = data['abbr']\n",
    "\n",
    "with open(f\"processed_onet_concepts_{model_name.replace('/', '_')}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    processed_concepts = json.load(f)\n",
    "\n",
    "# Initialize model (must be the same as used for generating embeddings)\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "THRESHOLD_NGRAM = 0.5\n",
    "\n",
    "# Compute embeddings for candidate phrases\n",
    "candidate_embeddings = model.encode(candidate_phrases, convert_to_numpy=True)\n",
    "\n",
    "# For each candidate phrase, compute similarity with both main and abbreviation embeddings.\n",
    "recognized_candidates_ngram = []\n",
    "for i, cand_emb in enumerate(candidate_embeddings):\n",
    "    # Compute similarity vectors for main and abbreviation parts.\n",
    "    sim_main = cosine_similarity([cand_emb], main_embeddings)[0]\n",
    "    sim_abbr = cosine_similarity([cand_emb], abbr_embeddings)[0]\n",
    "    # Choose the higher similarity per concept.\n",
    "    best_scores = np.maximum(sim_main, sim_abbr)\n",
    "    best_idx = best_scores.argmax()\n",
    "    best_score = best_scores[best_idx]\n",
    "\n",
    "    if best_score >= THRESHOLD_NGRAM:\n",
    "        concept = processed_concepts[best_idx]\n",
    "        # Determine which part (main or abbr) produced the highest score.\n",
    "        source = \"main\" if sim_main[best_idx] >= sim_abbr[best_idx] else \"abbr\"\n",
    "        phrase = candidate_phrases[i]\n",
    "        n_val = len(phrase.split())\n",
    "        tokens_phrase = phrase.split()\n",
    "        recognized_candidates_ngram.append(\n",
    "            (concept[\"name\"], concept[\"type\"], phrase,\n",
    "             best_score, n_val, tokens_phrase, source)\n",
    "        )\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Global Filtering of Overlapping N窶組rams\n",
    "# ---------------------------\n",
    "FILTER_SIMILARITY_THRESHOLD = 0.85\n",
    "recognized_candidates_ngram = sorted(\n",
    "    recognized_candidates_ngram, key=lambda x: x[3], reverse=True)\n",
    "global_used_words = set()\n",
    "filtered_candidates = []\n",
    "for candidate in recognized_candidates_ngram:\n",
    "    concept_name, concept_type, phrase, score, n_val, tokens_phrase, source = candidate\n",
    "    if any(token in global_used_words for token in tokens_phrase):\n",
    "        continue\n",
    "    filtered_candidates.append(candidate)\n",
    "    if score > FILTER_SIMILARITY_THRESHOLD:\n",
    "        global_used_words.update(tokens_phrase)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 5: Group and Print the Results\n",
    "# ---------------------------\n",
    "filtered_by_concept = {}\n",
    "for concept_name, concept_type, phrase, score, n_val, tokens_phrase, source in filtered_candidates:\n",
    "    filtered_by_concept.setdefault(\n",
    "        concept_name, {\"type\": concept_type, \"phrases\": []})\n",
    "    filtered_by_concept[concept_name][\"phrases\"].append(\n",
    "        (phrase, score, n_val, tokens_phrase, source))\n",
    "\n",
    "print(\"\\nGlobally Filtered Recognized Concepts using n窶組ram detection:\")\n",
    "print(\"=\" * 60)\n",
    "for concept, info in filtered_by_concept.items():\n",
    "    concept_type = info[\"type\"]\n",
    "    print(f\"Concept: {concept} ({concept_type})\")\n",
    "    for phrase, score, n_val, tokens_phrase, source in sorted(info[\"phrases\"], key=lambda x: x[1], reverse=True):\n",
    "        print(\n",
    "            f\"    Detected {n_val}-gram: '{phrase}' with similarity {score:.2f} (matched with {source} text)\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
