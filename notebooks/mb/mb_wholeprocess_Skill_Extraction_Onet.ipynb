{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "082f51f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\conda\\envs\\tnsr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph contains 1114 nodes and 450 edges.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "# Load and extract O*NET concepts as before\n",
        "with open(\"../OnetData/abbr_cleaned_IT_data_from_onet.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    onet_data = json.load(f)\n",
        "\n",
        "onet_skill_titles = set()\n",
        "onet_tech_names = set()\n",
        "\n",
        "for job in onet_data:\n",
        "    for tech_skill in job.get(\"technology_skills\", []):\n",
        "        if \"skill_title\" in tech_skill:\n",
        "            onet_skill_titles.add(tech_skill[\"skill_title\"])\n",
        "        for tech_item in tech_skill.get(\"technologies\", []):\n",
        "            onet_tech_names.add(tech_item[\"name\"])\n",
        "\n",
        "# Combine into a list of dictionaries\n",
        "onet_concepts = (\n",
        "    [{\"name\": title, \"type\": \"skill_title\"} for title in onet_skill_titles] +\n",
        "    [{\"name\": tech, \"type\": \"technology_name\"} for tech in onet_tech_names]\n",
        ")\n",
        "\n",
        "# Process each concept to separate the main text and the abbreviation\n",
        "processed_concepts = []\n",
        "for concept in onet_concepts:\n",
        "    full_text = concept[\"name\"]\n",
        "    # Get the main part (everything before the first parenthesis)\n",
        "    main_text = re.sub(r'\\s*\\(.*', '', full_text).strip()\n",
        "    # Extract abbreviation if available\n",
        "    abbr_match = re.search(r'\\((.*?)\\)', full_text)\n",
        "    abbr_text = abbr_match.group(1).strip() if abbr_match else \"\"\n",
        "    processed_concepts.append({\n",
        "        \"name\": full_text,\n",
        "        \"type\": concept[\"type\"],\n",
        "        \"main\": main_text,\n",
        "        \"abbr\": abbr_text\n",
        "    })\n",
        "\n",
        "# Initialize the model\n",
        "model_name = \"sentence-transformers/msmarco-distilbert-base-v4\"\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Create lists for the main texts and abbreviation texts\n",
        "main_texts = [item[\"main\"] for item in processed_concepts]\n",
        "abbr_texts = [item[\"abbr\"] for item in processed_concepts]\n",
        "\n",
        "# Generate embeddings for both parts\n",
        "main_embeddings = model.encode(main_texts, convert_to_numpy=True)\n",
        "abbr_embeddings = model.encode(abbr_texts, convert_to_numpy=True)\n",
        "\n",
        "# (Optional) Save dual embeddings along with the processed concepts for later use.\n",
        "np.savez(f\"onet_concept_embeddings_{model_name.replace('/', '_')}.npz\",\n",
        "         main=main_embeddings, abbr=abbr_embeddings)\n",
        "with open(f\"processed_onet_concepts_{model_name.replace('/', '_')}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_concepts, f, indent=4)\n",
        "\n",
        "# (Optional) Build a similarity graph using the main embeddings\n",
        "G = nx.Graph()\n",
        "for concept in processed_concepts:\n",
        "    G.add_node(concept[\"name\"], category=concept[\"type\"])\n",
        "\n",
        "similarity_matrix = cosine_similarity(main_embeddings)\n",
        "SIMILARITY_THRESHOLD = 0.7  # For graph creation only\n",
        "for i in range(len(main_texts)):\n",
        "    for j in range(i + 1, len(main_texts)):\n",
        "        if similarity_matrix[i][j] >= SIMILARITY_THRESHOLD:\n",
        "            G.add_edge(main_texts[i], main_texts[j],\n",
        "                       weight=similarity_matrix[i][j])\n",
        "\n",
        "print(f\"Graph contains {len(G.nodes)} nodes and {len(G.edges)} edges.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db42bf8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Zinou\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Zinou\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Globally Filtered Recognized Concepts using nâ€‘gram detection:\n",
            "============================================================\n",
            "Concept: Microsoft Azure software (Azure) (technology_name)\n",
            "    Detected 1-gram: 'azure' with similarity 1.00 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: NoSQL (technology_name)\n",
            "    Detected 1-gram: 'nosql' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Apache (technology_name)\n",
            "    Detected 1-gram: 'apache' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Python (technology_name)\n",
            "    Detected 1-gram: 'python' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Apache Hadoop (Hadoop) (technology_name)\n",
            "    Detected 1-gram: 'hadoop' with similarity 1.00 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: TensorFlow (TF) (technology_name)\n",
            "    Detected 1-gram: 'tensorflow' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Amazon Web Services SageMaker (SageMaker) (technology_name)\n",
            "    Detected 1-gram: 'sagemaker' with similarity 1.00 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: MongoDB (technology_name)\n",
            "    Detected 1-gram: 'mongodb' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: PyTorch (technology_name)\n",
            "    Detected 1-gram: 'pytorch' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Kubernetes (K8s) (technology_name)\n",
            "    Detected 1-gram: 'kubernetes' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: MySQL (technology_name)\n",
            "    Detected 1-gram: 'mysql' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Amazon Web Services software (AWS) (technology_name)\n",
            "    Detected 1-gram: 'aws' with similarity 1.00 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Docker (technology_name)\n",
            "    Detected 1-gram: 'docker' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: PostgreSQL (Postgres) (technology_name)\n",
            "    Detected 1-gram: 'postgresql' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: IBM Power Systems software (Power) (technology_name)\n",
            "    Detected 1-gram: 'power' with similarity 1.00 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Tableau (technology_name)\n",
            "    Detected 1-gram: 'tableau' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Structured query language (SQL) (technology_name)\n",
            "    Detected 1-gram: 'sql' with similarity 1.00 (matched with abbr text)\n",
            "    Detected 3-gram: 'structured query language' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: R (technology_name)\n",
            "    Detected 1-gram: 'r' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Git (technology_name)\n",
            "    Detected 1-gram: 'git' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Microsoft (technology_name)\n",
            "    Detected 1-gram: 'microsoft' with similarity 1.00 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Version control software (Version Ctrl) (technology_name)\n",
            "    Detected 2-gram: 'version control' with similarity 0.90 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Data visualization software (Data Viz SW) (technology_name)\n",
            "    Detected 2-gram: 'data visualization' with similarity 0.80 (matched with main text)\n",
            "    Detected 3-gram: 'data visualization using' with similarity 0.74 (matched with main text)\n",
            "    Detected 3-gram: 'skilled data visualization' with similarity 0.58 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Google Android (Android) (technology_name)\n",
            "    Detected 1-gram: 'google' with similarity 0.79 (matched with main text)\n",
            "    Detected 2-gram: 'google cloud' with similarity 0.66 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Relational database software (RDB) (technology_name)\n",
            "    Detected 2-gram: 'relational databases' with similarity 0.76 (matched with main text)\n",
            "    Detected 1-gram: 'relational' with similarity 0.61 (matched with main text)\n",
            "    Detected 3-gram: 'experience relational databases' with similarity 0.54 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Data modeling software (Data Model SW) (technology_name)\n",
            "    Detected 3-gram: 'modeling data analysis' with similarity 0.74 (matched with main text)\n",
            "    Detected 2-gram: 'modeling data' with similarity 0.72 (matched with main text)\n",
            "    Detected 3-gram: 'statistical modeling data' with similarity 0.64 (matched with main text)\n",
            "    Detected 1-gram: 'modeling' with similarity 0.62 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Microsoft Project (MS Project) (technology_name)\n",
            "    Detected 1-gram: 'project' with similarity 0.74 (matched with main text)\n",
            "    Detected 2-gram: 'agile project' with similarity 0.51 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Project management software (PM SW) (skill_title)\n",
            "    Detected 2-gram: 'project management' with similarity 0.73 (matched with main text)\n",
            "    Detected 3-gram: 'agile project management' with similarity 0.62 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Business intelligence system software (BI SW) (technology_name)\n",
            "    Detected 1-gram: 'bi' with similarity 0.72 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: data analysis software (skill_title)\n",
            "    Detected 2-gram: 'data analysis' with similarity 0.71 (matched with main text)\n",
            "    Detected 2-gram: 'data tools' with similarity 0.71 (matched with main text)\n",
            "    Detected 3-gram: 'data analysis proficient' with similarity 0.60 (matched with main text)\n",
            "    Detected 3-gram: 'big data tools' with similarity 0.57 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Web framework software (Web FW) (technology_name)\n",
            "    Detected 1-gram: 'frameworks' with similarity 0.70 (matched with main text)\n",
            "    Detected 2-gram: 'learning frameworks' with similarity 0.58 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Customer information control system CICS (CICS) (technology_name)\n",
            "    Detected 1-gram: 'cicd' with similarity 0.68 (matched with abbr text)\n",
            "    Detected 2-gram: 'cicd pipelines' with similarity 0.51 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: The MathWorks MATLAB (MATLAB) (technology_name)\n",
            "    Detected 1-gram: 'matplotlib' with similarity 0.68 (matched with abbr text)\n",
            "    Detected 3-gram: 'matplotlib familiar cloud' with similarity 0.64 (matched with abbr text)\n",
            "    Detected 2-gram: 'matplotlib familiar' with similarity 0.62 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: analysis software (skill_title)\n",
            "    Detected 1-gram: 'analysis' with similarity 0.67 (matched with main text)\n",
            "    Detected 3-gram: 'analysis proficient machine' with similarity 0.60 (matched with main text)\n",
            "    Detected 2-gram: 'analysis proficient' with similarity 0.54 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Platform as a service PaaS (PaaS) (technology_name)\n",
            "    Detected 1-gram: 'platform' with similarity 0.66 (matched with main text)\n",
            "    Detected 1-gram: 'platforms' with similarity 0.60 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Database management systems (DBMS) (technology_name)\n",
            "    Detected 1-gram: 'databases' with similarity 0.66 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Program testing software (Testing SW) (skill_title)\n",
            "    Detected 1-gram: 'testing' with similarity 0.66 (matched with abbr text)\n",
            "    Detected 2-gram: 'ab testing' with similarity 0.54 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Google Analytics (Analytics) (technology_name)\n",
            "    Detected 3-gram: 'google cloud ai' with similarity 0.65 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Statistical software (Stats SW) (technology_name)\n",
            "    Detected 2-gram: 'statistical modeling' with similarity 0.64 (matched with main text)\n",
            "    Detected 1-gram: 'statistical' with similarity 0.60 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Scikit-learn (Sklearn) (technology_name)\n",
            "    Detected 1-gram: 'scikitlearn' with similarity 0.64 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Other Technology Skills (Other Tech SW) (skill_title)\n",
            "    Detected 1-gram: 'skills' with similarity 0.64 (matched with main text)\n",
            "    Detected 2-gram: 'skills include' with similarity 0.64 (matched with main text)\n",
            "    Detected 3-gram: 'skills include stakeholder' with similarity 0.52 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Software development tools (Dev Tools) (technology_name)\n",
            "    Detected 1-gram: 'tools' with similarity 0.63 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Amazon Web Services CloudFormation (CloudFormation) (technology_name)\n",
            "    Detected 1-gram: 'cloud' with similarity 0.63 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Collaborative editing software (Collab Edit SW) (technology_name)\n",
            "    Detected 2-gram: 'collaboration agile' with similarity 0.60 (matched with main text)\n",
            "    Detected 3-gram: 'collaboration agile project' with similarity 0.56 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Oracle E-Business Suite Financials (EBS) (technology_name)\n",
            "    Detected 1-gram: 'eg' with similarity 0.60 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Oracle software (Oracle Cloud) (technology_name)\n",
            "    Detected 2-gram: 'cloud platforms' with similarity 0.59 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Oracle Database (Oracle DB) (technology_name)\n",
            "    Detected 3-gram: 'relational databases eg' with similarity 0.59 (matched with main text)\n",
            "    Detected 2-gram: 'databases eg' with similarity 0.54 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Word processing software (Word SW) (skill_title)\n",
            "    Detected 1-gram: 'processing' with similarity 0.55 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Citrix cloud computing software (Citrix Cloud) (technology_name)\n",
            "    Detected 2-gram: 'cloud ai' with similarity 0.53 (matched with main text)\n",
            "    Detected 3-gram: 'cloud ai platform' with similarity 0.53 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Management information systems MIS (MIS) (technology_name)\n",
            "    Detected 1-gram: 'management' with similarity 0.52 (matched with main text)\n",
            "------------------------------------------------------------\n",
            "Concept: Oracle Business Intelligence Suite (Oracle BI) (technology_name)\n",
            "    Detected 2-gram: 'bi tools' with similarity 0.52 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: IBM InfoSphere DataStage (DataStage) (technology_name)\n",
            "    Detected 1-gram: 'data' with similarity 0.52 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Informatica Data Explorer (Data Explorer) (technology_name)\n",
            "    Detected 2-gram: 'data scientist' with similarity 0.51 (matched with abbr text)\n",
            "------------------------------------------------------------\n",
            "Concept: Qualys Platform (Qualys) (technology_name)\n",
            "    Detected 2-gram: 'ai platform' with similarity 0.50 (matched with main text)\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stop words and custom filter words.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "custom_filter_words = {'additionally', 'also', 'furthermore',\n",
        "                       'moreover', 'including', 'like', 'career', 'etc'}\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Lowercase and remove punctuation.\n",
        "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join([token for token in tokens if token not in stop_words])\n",
        "\n",
        "\n",
        "def is_meaningful(phrase):\n",
        "    tokens = [t.lower() for t in word_tokenize(phrase) if t.isalpha()]\n",
        "    if not tokens:\n",
        "        return False\n",
        "    if any(token in custom_filter_words for token in tokens):\n",
        "        return False\n",
        "    if len(tokens) == 1 and tokens[0] in stop_words:\n",
        "        return False\n",
        "    if sum(1 for t in tokens if t in stop_words)/len(tokens) > 0.5:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 1: Process Resume Text\n",
        "# ---------------------------\n",
        "# long_text = \"\"\"\n",
        "# I have extensive experience in data analysis and have worked with a variety of technologies including Microsoft SQL Server,\n",
        "# Python, cloud computing platforms like AWS, and I am proficient with machine learning techniques. My background also includes\n",
        "# developing user interfaces with modern tools. Additionally, I have hands-on experience with business intelligence and\n",
        "# graphical user interface design.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"Throughout my career, I have developed expertise in backend development using Python and Node.js. I have built robust REST APIs and worked with various databases including MySQL and MongoDB. My experience also extends to cloud services like AWS and Azure,\n",
        "# enabling me to deploy scalable applications.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"In my career as a full-stack developer, I have worked extensively with JavaScript, React, and Node.js to create responsive web applications. I have also integrated MySQL and PostgreSQL databases into several projects, while leveraging cloud platforms like AWS and Google Cloud to deploy and manage applications.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"I have worked as a data engineer and have extensive experience with Python and Apache Spark to process large datasets. My work also involved using cloud-based storage solutions like Amazon S3 and Google BigQuery, as well as using Docker for containerization and Kubernetes for orchestration.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"As a DevOps engineer, I have automated CI/CD pipelines using Jenkins and GitLab CI. I am proficient in cloud services such as AWS and Azure, where I have deployed applications and managed resources like EC2 instances and databases. I also have experience with containerization tools such as Docker and Kubernetes.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"My experience in cybersecurity includes working with various firewalls, encryption algorithms, and intrusion detection systems (IDS). I am proficient with tools like Wireshark, Metasploit, and AWS Security Hub, and I have experience using Kubernetes for securing containerized applications.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"I specialize in machine learning, with extensive experience using Python libraries such as TensorFlow and scikit-learn. I have built various models for predictive analytics, and my cloud experience includes working with AWS SageMaker for training models and deploying them into production.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"As a system administrator, I have managed Linux and Windows servers, focusing on performance optimization and security. I have experience with AWS EC2 instances, configuring firewalls, and using Ansible for configuration management and automation.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"I have worked as a mobile app developer with expertise in building cross-platform apps using Flutter and React Native. I am experienced in integrating REST APIs and managing cloud databases like Firebase and MongoDB for seamless app functionality.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"In my role as a cloud architect, I have designed scalable cloud infrastructures using AWS, Azure, and Google Cloud Platform. I have also implemented Infrastructure as Code (IaC) using tools like Terraform and AWS CloudFormation to automate deployments and manage resources.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"With a background in UI/UX design, I have used tools like Figma and Adobe XD to design user-centric interfaces. I also have experience in front-end development using JavaScript, HTML, and CSS, along with frameworks like Angular and Vue.js for creating dynamic and interactive web applications.\n",
        "# \"\"\"\n",
        "# long_text = \"\"\"As a network engineer, I have designed and implemented large-scale network solutions using Cisco devices. I am proficient in configuring routers, switches, and firewalls, and have experience with network monitoring tools like SolarWinds and Wireshark.\n",
        "# \"\"\"\n",
        "long_text = \"\"\"Iâ€™m a data scientist with expertise in Python, R, and SQL (Structured Query Language) for statistical modeling and data analysis. Proficient in machine learning frameworks like TensorFlow, PyTorch, and scikit-learn, and experienced with big data tools such as Apache Spark (Spark) and Hadoop.\n",
        "\n",
        "Skilled in data visualization using Tableau, Power BI, and Matplotlib, and familiar with cloud platforms like AWS SageMaker, Google Cloud AI Platform, and Microsoft Azure Machine Learning.\n",
        "\n",
        "Strong background in A/B testing, natural language processing (NLP), and deep learning. Comfortable with version control tools like Git and CI/CD pipelines.\n",
        "\n",
        "Additional experience with relational databases (e.g., PostgreSQL, MySQL), NoSQL databases (e.g., MongoDB), and BI tools like Looker. Familiar with Docker for containerization and Kubernetes for orchestration.\n",
        "\n",
        "Soft skills include stakeholder communication, cross-functional collaboration, and agile project management.\n",
        "\"\"\"\n",
        "\n",
        "cleaned_full_text = clean_text(long_text)\n",
        "tokens_clean = word_tokenize(cleaned_full_text)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 2: Generate Candidate Phrases using nâ€‘grams\n",
        "# ---------------------------\n",
        "candidate_phrases = []\n",
        "for n in [3, 2, 1]:\n",
        "    for gram in ngrams(tokens_clean, n):\n",
        "        phrase = \" \".join(gram)\n",
        "        if phrase.strip() and is_meaningful(phrase):\n",
        "            candidate_phrases.append(phrase)\n",
        "candidate_phrases = list(set(candidate_phrases))  # Remove duplicates\n",
        "\n",
        "# ---------------------------\n",
        "# Step 3: Improved Matching Against Concepts\n",
        "# ---------------------------\n",
        "# Load the pre-generated dual embeddings and processed concepts.\n",
        "model_name = \"sentence-transformers/msmarco-distilbert-base-v4\"\n",
        "data = np.load(f\"onet_concept_embeddings_{model_name.replace('/', '_')}.npz\")\n",
        "main_embeddings = data['main']\n",
        "abbr_embeddings = data['abbr']\n",
        "\n",
        "with open(f\"processed_onet_concepts_{model_name.replace('/', '_')}.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    processed_concepts = json.load(f)\n",
        "\n",
        "# Initialize model (must be the same as used for generating embeddings)\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "THRESHOLD_NGRAM = 0.5\n",
        "\n",
        "# Compute embeddings for candidate phrases\n",
        "candidate_embeddings = model.encode(candidate_phrases, convert_to_numpy=True)\n",
        "\n",
        "# For each candidate phrase, compute similarity with both main and abbreviation embeddings.\n",
        "recognized_candidates_ngram = []\n",
        "for i, cand_emb in enumerate(candidate_embeddings):\n",
        "    # Compute similarity vectors for main and abbreviation parts.\n",
        "    sim_main = cosine_similarity([cand_emb], main_embeddings)[0]\n",
        "    sim_abbr = cosine_similarity([cand_emb], abbr_embeddings)[0]\n",
        "    # Choose the higher similarity per concept.\n",
        "    best_scores = np.maximum(sim_main, sim_abbr)\n",
        "    best_idx = best_scores.argmax()\n",
        "    best_score = best_scores[best_idx]\n",
        "\n",
        "    if best_score >= THRESHOLD_NGRAM:\n",
        "        concept = processed_concepts[best_idx]\n",
        "        # Determine which part (main or abbr) produced the highest score.\n",
        "        source = \"main\" if sim_main[best_idx] >= sim_abbr[best_idx] else \"abbr\"\n",
        "        phrase = candidate_phrases[i]\n",
        "        n_val = len(phrase.split())\n",
        "        tokens_phrase = phrase.split()\n",
        "        recognized_candidates_ngram.append(\n",
        "            (concept[\"name\"], concept[\"type\"], phrase,\n",
        "             best_score, n_val, tokens_phrase, source)\n",
        "        )\n",
        "\n",
        "# ---------------------------\n",
        "# Step 4: Global Filtering of Overlapping Nâ€‘grams\n",
        "# ---------------------------\n",
        "FILTER_SIMILARITY_THRESHOLD = 0.85\n",
        "recognized_candidates_ngram = sorted(\n",
        "    recognized_candidates_ngram, key=lambda x: x[3], reverse=True)\n",
        "global_used_words = set()\n",
        "filtered_candidates = []\n",
        "for candidate in recognized_candidates_ngram:\n",
        "    concept_name, concept_type, phrase, score, n_val, tokens_phrase, source = candidate\n",
        "    if any(token in global_used_words for token in tokens_phrase):\n",
        "        continue\n",
        "    filtered_candidates.append(candidate)\n",
        "    if score > FILTER_SIMILARITY_THRESHOLD:\n",
        "        global_used_words.update(tokens_phrase)\n",
        "\n",
        "# ---------------------------\n",
        "# Step 5: Group and Print the Results\n",
        "# ---------------------------\n",
        "filtered_by_concept = {}\n",
        "for concept_name, concept_type, phrase, score, n_val, tokens_phrase, source in filtered_candidates:\n",
        "    filtered_by_concept.setdefault(\n",
        "        concept_name, {\"type\": concept_type, \"phrases\": []})\n",
        "    filtered_by_concept[concept_name][\"phrases\"].append(\n",
        "        (phrase, score, n_val, tokens_phrase, source))\n",
        "\n",
        "print(\"\\nGlobally Filtered Recognized Concepts using nâ€‘gram detection:\")\n",
        "print(\"=\" * 60)\n",
        "for concept, info in filtered_by_concept.items():\n",
        "    concept_type = info[\"type\"]\n",
        "    print(f\"Concept: {concept} ({concept_type})\")\n",
        "    for phrase, score, n_val, tokens_phrase, source in sorted(info[\"phrases\"], key=lambda x: x[1], reverse=True):\n",
        "        print(\n",
        "            f\"    Detected {n_val}-gram: '{phrase}' with similarity {score:.2f} (matched with {source} text)\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6e92debd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[DEBUG] Precomputing job embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/38 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Job 'Actuaries' has 66 terms\n",
            "[DEBUG] Job 'Bioinformatics Technicians' has 78 terms\n",
            "[DEBUG] Job 'Biostatisticians' has 71 terms\n",
            "[DEBUG] Job 'Blockchain Engineers' has 77 terms\n",
            "[DEBUG] Job 'Business Intelligence Analysts' has 251 terms\n",
            "[DEBUG] Job 'Clinical Data Managers' has 70 terms\n",
            "[DEBUG] Job 'Computer and Information Research Scientists' has 161 terms\n",
            "[DEBUG] Job 'Computer Network Architects' has 322 terms\n",
            "[DEBUG] Job 'Computer Network Support Specialists' has 171 terms\n",
            "[DEBUG] Job 'Computer Programmers' has 265 terms\n",
            "[DEBUG] Job 'Computer Systems Analysts' has 332 terms\n",
            "[DEBUG] Job 'Computer Systems Engineers/Architects' has 317 terms\n",
            "[DEBUG] Job 'Computer User Support Specialists' has 351 terms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 14/38 [00:00<00:00, 138.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Job 'Data Scientists' has 110 terms\n",
            "[DEBUG] Job 'Data Warehousing Specialists' has 180 terms\n",
            "[DEBUG] Job 'Database Administrators' has 301 terms\n",
            "[DEBUG] Job 'Database Architects' has 297 terms\n",
            "[DEBUG] Job 'Digital Forensics Analysts' has 119 terms\n",
            "[DEBUG] Job 'Document Management Specialists' has 134 terms\n",
            "[DEBUG] Job 'Geographic Information Systems Technologists and Technicians' has 158 terms\n",
            "[DEBUG] Job 'Health Informatics Specialists' has 82 terms\n",
            "[DEBUG] Job 'Information Security Analysts' has 282 terms\n",
            "[DEBUG] Job 'Information Security Engineers' has 148 terms\n",
            "[DEBUG] Job 'Information Technology Project Managers' has 309 terms\n",
            "[DEBUG] Job 'Mathematicians' has 81 terms\n",
            "[DEBUG] Job 'Network and Computer Systems Administrators' has 310 terms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<00:00, 141.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Job 'Operations Research Analysts' has 156 terms\n",
            "[DEBUG] Job 'Penetration Testers' has 104 terms\n",
            "[DEBUG] Job 'Software Developers' has 362 terms\n",
            "[DEBUG] Job 'Software Quality Assurance Analysts and Testers' has 358 terms\n",
            "[DEBUG] Job 'Statisticians' has 73 terms\n",
            "[DEBUG] Job 'Telecommunications Engineering Specialists' has 106 terms\n",
            "[DEBUG] Job 'Video Game Designers' has 91 terms\n",
            "[DEBUG] Job 'Web Administrators' has 176 terms\n",
            "[DEBUG] Job 'Web and Digital Interface Designers' has 273 terms\n",
            "[DEBUG] Job 'Web Developers' has 283 terms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# Modified Job Embedding Precomputation (without demand_percentage)\n",
        "# ---------------------------\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load processed concepts and embeddings\n",
        "model_name = \"sentence-transformers/msmarco-distilbert-base-v4\"\n",
        "data = np.load(f\"onet_concept_embeddings_{model_name.replace('/', '_')}.npz\")\n",
        "main_emb = data['main']\n",
        "abbr_emb = data['abbr']\n",
        "\n",
        "with open(f\"processed_onet_concepts_{model_name.replace('/', '_')}.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    processed_concepts = json.load(f)\n",
        "\n",
        "# Load job data\n",
        "with open(\"../OnetData/abbr_cleaned_IT_data_from_onet.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    jobs = json.load(f)\n",
        "\n",
        "# Precompute job embeddings (equal weighting)\n",
        "print(\"\\n[DEBUG] Precomputing job embeddings...\")\n",
        "job_embeddings = []\n",
        "job_titles = []\n",
        "\n",
        "for job in tqdm(jobs):\n",
        "    terms = []\n",
        "\n",
        "    # Collect all skill titles and technology names\n",
        "    for tech_skill in job.get(\"technology_skills\", []):\n",
        "        if skill_title := tech_skill.get(\"skill_title\", \"\"):\n",
        "            terms.append(skill_title)\n",
        "\n",
        "        for tech in tech_skill.get(\"technologies\", []):\n",
        "            if tech_name := tech.get(\"name\", \"\"):\n",
        "                terms.append(tech_name)\n",
        "\n",
        "    # Aggregate term embeddings with equal weight\n",
        "    job_vec = np.zeros_like(main_emb[0])\n",
        "    total_terms = 0\n",
        "\n",
        "    for term_name in terms:\n",
        "        concept = next(\n",
        "            (c for c in processed_concepts if c['name'] == term_name), None)\n",
        "        if not concept:\n",
        "            continue\n",
        "\n",
        "        idx = processed_concepts.index(concept)\n",
        "        m_emb = main_emb[idx]\n",
        "        a_emb = abbr_emb[idx]\n",
        "\n",
        "        # Use average of main and abbr if available\n",
        "        term_vec = (m_emb + a_emb)/2 if concept['abbr'] else m_emb\n",
        "\n",
        "        job_vec += term_vec\n",
        "        total_terms += 1\n",
        "\n",
        "    if total_terms > 0:\n",
        "        job_vec /= total_terms\n",
        "        print(f\"[DEBUG] Job '{job.get('title', '')}' has {total_terms} terms\")\n",
        "\n",
        "    job_embeddings.append(job_vec)\n",
        "    job_titles.append(job.get(\"title\", \"\"))\n",
        "\n",
        "# Save precomputed data\n",
        "np.save(\"job_embeddings.npy\", np.array(job_embeddings))\n",
        "with open(\"job_titles.json\", \"w\") as f:\n",
        "    json.dump(job_titles, f)\n",
        "\n",
        "# ---------------------------\n",
        "# Modified Recommendation Function\n",
        "# ---------------------------\n",
        "\n",
        "\n",
        "def recommend_jobs(filtered_candidates, top_n=5):\n",
        "    print(\"\\n[DEBUG] Starting recommendation process...\")\n",
        "\n",
        "    # Group concepts by name and keep highest score\n",
        "    concept_scores = {}\n",
        "    for candidate in filtered_candidates:\n",
        "        name = candidate[0]\n",
        "        score = candidate[3]\n",
        "        c_type = candidate[1]\n",
        "        if name not in concept_scores or score > concept_scores[name]['score']:\n",
        "            concept_scores[name] = {'score': score, 'type': c_type}\n",
        "\n",
        "    print(\"[DEBUG] Unique concepts with max scores:\")\n",
        "    for name, data in concept_scores.items():\n",
        "        print(f\"  - {name} ({data['type']}): {data['score']:.2f}\")\n",
        "\n",
        "    # Create weighted user embedding\n",
        "    user_vec = np.zeros_like(job_embeddings[0])\n",
        "    total_weight = 0.0\n",
        "    # Weight tech higher than skills\n",
        "    type_weights = {'technology_name': 1.0, 'skill_title': 0.7}\n",
        "\n",
        "    for name, data in concept_scores.items():\n",
        "        concept = next(\n",
        "            (c for c in processed_concepts if c['name'] == name), None)\n",
        "        if not concept:\n",
        "            continue\n",
        "\n",
        "        # Get type-based weight\n",
        "        weight = data['score'] * type_weights.get(data['type'], 0.5)\n",
        "\n",
        "        # Get embeddings\n",
        "        idx = processed_concepts.index(concept)\n",
        "        m_emb = main_emb[idx]\n",
        "        a_emb = abbr_emb[idx]\n",
        "        term_vec = (m_emb + a_emb)/2 if concept['abbr'] else m_emb\n",
        "\n",
        "        # Apply non-linear scoring (emphasize >0.8 matches)\n",
        "        boosted_weight = weight * np.tanh(weight * 3)  # Boost higher scores\n",
        "        contribution = term_vec * boosted_weight\n",
        "\n",
        "        user_vec += contribution\n",
        "        total_weight += boosted_weight\n",
        "\n",
        "        print(f\"[DEBUG] {name[:30]:<30} | Type: {data['type']:16} | \"\n",
        "              f\"Raw: {data['score']:.2f} | Boosted: {boosted_weight:.2f} | \"\n",
        "              f\"Vec Norm: {np.linalg.norm(contribution):.2f}\")\n",
        "\n",
        "    if total_weight > 0:\n",
        "        user_vec /= total_weight\n",
        "        print(\n",
        "            f\"\\n[DEBUG] Final user vector norm: {np.linalg.norm(user_vec):.2f}\")\n",
        "    else:\n",
        "        print(\"[WARNING] No valid concepts found - using zero vector\")\n",
        "\n",
        "    # Calculate similarities\n",
        "    sims = cosine_similarity([user_vec], job_embeddings)[0]\n",
        "    top_indices = np.argsort(sims)[-top_n:][::-1]\n",
        "\n",
        "    print(\"\\n[DEBUG] Top matches:\")\n",
        "    for idx in top_indices:\n",
        "        print(f\"  {sims[idx]:.4f} - {job_titles[idx]}\")\n",
        "\n",
        "    return [(job_titles[i], sims[i]) for i in top_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "65c8b4e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[DEBUG] Starting recommendation process...\n",
            "[DEBUG] Unique concepts with max scores:\n",
            "  - Microsoft Azure software (Azure) (technology_name): 1.00\n",
            "  - NoSQL (technology_name): 1.00\n",
            "  - Apache (technology_name): 1.00\n",
            "  - Python (technology_name): 1.00\n",
            "  - Apache Hadoop (Hadoop) (technology_name): 1.00\n",
            "  - TensorFlow (TF) (technology_name): 1.00\n",
            "  - Amazon Web Services SageMaker (SageMaker) (technology_name): 1.00\n",
            "  - MongoDB (technology_name): 1.00\n",
            "  - PyTorch (technology_name): 1.00\n",
            "  - Kubernetes (K8s) (technology_name): 1.00\n",
            "  - MySQL (technology_name): 1.00\n",
            "  - Amazon Web Services software (AWS) (technology_name): 1.00\n",
            "  - Docker (technology_name): 1.00\n",
            "  - PostgreSQL (Postgres) (technology_name): 1.00\n",
            "  - IBM Power Systems software (Power) (technology_name): 1.00\n",
            "  - Tableau (technology_name): 1.00\n",
            "  - Structured query language (SQL) (technology_name): 1.00\n",
            "  - R (technology_name): 1.00\n",
            "  - Git (technology_name): 1.00\n",
            "  - Microsoft (technology_name): 1.00\n",
            "  - Version control software (Version Ctrl) (technology_name): 0.90\n",
            "  - Data visualization software (Data Viz SW) (technology_name): 0.80\n",
            "  - Google Android (Android) (technology_name): 0.79\n",
            "  - Relational database software (RDB) (technology_name): 0.76\n",
            "  - Data modeling software (Data Model SW) (technology_name): 0.74\n",
            "  - Microsoft Project (MS Project) (technology_name): 0.74\n",
            "  - Project management software (PM SW) (skill_title): 0.73\n",
            "  - Business intelligence system software (BI SW) (technology_name): 0.72\n",
            "  - data analysis software (skill_title): 0.71\n",
            "  - Web framework software (Web FW) (technology_name): 0.70\n",
            "  - Customer information control system CICS (CICS) (technology_name): 0.68\n",
            "  - The MathWorks MATLAB (MATLAB) (technology_name): 0.68\n",
            "  - analysis software (skill_title): 0.67\n",
            "  - Platform as a service PaaS (PaaS) (technology_name): 0.66\n",
            "  - Database management systems (DBMS) (technology_name): 0.66\n",
            "  - Program testing software (Testing SW) (skill_title): 0.66\n",
            "  - Google Analytics (Analytics) (technology_name): 0.65\n",
            "  - Statistical software (Stats SW) (technology_name): 0.64\n",
            "  - Scikit-learn (Sklearn) (technology_name): 0.64\n",
            "  - Other Technology Skills (Other Tech SW) (skill_title): 0.64\n",
            "  - Software development tools (Dev Tools) (technology_name): 0.63\n",
            "  - Amazon Web Services CloudFormation (CloudFormation) (technology_name): 0.63\n",
            "  - Collaborative editing software (Collab Edit SW) (technology_name): 0.60\n",
            "  - Oracle E-Business Suite Financials (EBS) (technology_name): 0.60\n",
            "  - Oracle software (Oracle Cloud) (technology_name): 0.59\n",
            "  - Oracle Database (Oracle DB) (technology_name): 0.59\n",
            "  - Word processing software (Word SW) (skill_title): 0.55\n",
            "  - Citrix cloud computing software (Citrix Cloud) (technology_name): 0.53\n",
            "  - Management information systems MIS (MIS) (technology_name): 0.52\n",
            "  - Oracle Business Intelligence Suite (Oracle BI) (technology_name): 0.52\n",
            "  - IBM InfoSphere DataStage (DataStage) (technology_name): 0.52\n",
            "  - Informatica Data Explorer (Data Explorer) (technology_name): 0.51\n",
            "  - Qualys Platform (Qualys) (technology_name): 0.50\n",
            "[DEBUG] Microsoft Azure software (Azur | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 13.12\n",
            "[DEBUG] NoSQL                          | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 13.77\n",
            "[DEBUG] Apache                         | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 14.14\n",
            "[DEBUG] Python                         | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 14.11\n",
            "[DEBUG] Apache Hadoop (Hadoop)         | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 12.69\n",
            "[DEBUG] TensorFlow (TF)                | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 10.43\n",
            "[DEBUG] Amazon Web Services SageMaker  | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 12.75\n",
            "[DEBUG] MongoDB                        | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 13.85\n",
            "[DEBUG] PyTorch                        | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 12.37\n",
            "[DEBUG] Kubernetes (K8s)               | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 9.68\n",
            "[DEBUG] MySQL                          | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 12.53\n",
            "[DEBUG] Amazon Web Services software ( | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 10.73\n",
            "[DEBUG] Docker                         | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 13.65\n",
            "[DEBUG] PostgreSQL (Postgres)          | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 11.78\n",
            "[DEBUG] IBM Power Systems software (Po | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 11.85\n",
            "[DEBUG] Tableau                        | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 14.07\n",
            "[DEBUG] Structured query language (SQL | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 12.47\n",
            "[DEBUG] R                              | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 14.13\n",
            "[DEBUG] Git                            | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 13.37\n",
            "[DEBUG] Microsoft                      | Type: technology_name  | Raw: 1.00 | Boosted: 1.00 | Vec Norm: 14.40\n",
            "[DEBUG] Version control software (Vers | Type: technology_name  | Raw: 0.90 | Boosted: 0.89 | Vec Norm: 10.19\n",
            "[DEBUG] Data visualization software (D | Type: technology_name  | Raw: 0.80 | Boosted: 0.78 | Vec Norm: 9.00\n",
            "[DEBUG] Google Android (Android)       | Type: technology_name  | Raw: 0.79 | Boosted: 0.78 | Vec Norm: 10.58\n",
            "[DEBUG] Relational database software ( | Type: technology_name  | Raw: 0.76 | Boosted: 0.75 | Vec Norm: 8.68\n",
            "[DEBUG] Data modeling software (Data M | Type: technology_name  | Raw: 0.74 | Boosted: 0.73 | Vec Norm: 8.99\n",
            "[DEBUG] Microsoft Project (MS Project) | Type: technology_name  | Raw: 0.74 | Boosted: 0.72 | Vec Norm: 9.66\n",
            "[DEBUG] Project management software (P | Type: skill_title      | Raw: 0.73 | Boosted: 0.46 | Vec Norm: 5.05\n",
            "[DEBUG] Business intelligence system s | Type: technology_name  | Raw: 0.72 | Boosted: 0.71 | Vec Norm: 7.20\n",
            "[DEBUG] data analysis software         | Type: skill_title      | Raw: 0.71 | Boosted: 0.45 | Vec Norm: 6.20\n",
            "[DEBUG] Web framework software (Web FW | Type: technology_name  | Raw: 0.70 | Boosted: 0.68 | Vec Norm: 8.25\n",
            "[DEBUG] Customer information control s | Type: technology_name  | Raw: 0.68 | Boosted: 0.66 | Vec Norm: 8.65\n",
            "[DEBUG] The MathWorks MATLAB (MATLAB)  | Type: technology_name  | Raw: 0.68 | Boosted: 0.65 | Vec Norm: 8.41\n",
            "[DEBUG] analysis software              | Type: skill_title      | Raw: 0.67 | Boosted: 0.42 | Vec Norm: 6.09\n",
            "[DEBUG] Platform as a service PaaS (Pa | Type: technology_name  | Raw: 0.66 | Boosted: 0.64 | Vec Norm: 7.90\n",
            "[DEBUG] Database management systems (D | Type: technology_name  | Raw: 0.66 | Boosted: 0.64 | Vec Norm: 7.59\n",
            "[DEBUG] Program testing software (Test | Type: skill_title      | Raw: 0.66 | Boosted: 0.41 | Vec Norm: 5.11\n",
            "[DEBUG] Google Analytics (Analytics)   | Type: technology_name  | Raw: 0.65 | Boosted: 0.62 | Vec Norm: 8.63\n",
            "[DEBUG] Statistical software (Stats SW | Type: technology_name  | Raw: 0.64 | Boosted: 0.62 | Vec Norm: 7.15\n",
            "[DEBUG] Scikit-learn (Sklearn)         | Type: technology_name  | Raw: 0.64 | Boosted: 0.61 | Vec Norm: 6.21\n",
            "[DEBUG] Other Technology Skills (Other | Type: skill_title      | Raw: 0.64 | Boosted: 0.39 | Vec Norm: 4.59\n",
            "[DEBUG] Software development tools (De | Type: technology_name  | Raw: 0.63 | Boosted: 0.61 | Vec Norm: 7.88\n",
            "[DEBUG] Amazon Web Services CloudForma | Type: technology_name  | Raw: 0.63 | Boosted: 0.60 | Vec Norm: 7.27\n",
            "[DEBUG] Collaborative editing software | Type: technology_name  | Raw: 0.60 | Boosted: 0.57 | Vec Norm: 6.62\n",
            "[DEBUG] Oracle E-Business Suite Financ | Type: technology_name  | Raw: 0.60 | Boosted: 0.56 | Vec Norm: 6.35\n",
            "[DEBUG] Oracle software (Oracle Cloud) | Type: technology_name  | Raw: 0.59 | Boosted: 0.55 | Vec Norm: 7.28\n",
            "[DEBUG] Oracle Database (Oracle DB)    | Type: technology_name  | Raw: 0.59 | Boosted: 0.55 | Vec Norm: 7.61\n",
            "[DEBUG] Word processing software (Word | Type: skill_title      | Raw: 0.55 | Boosted: 0.32 | Vec Norm: 3.40\n",
            "[DEBUG] Citrix cloud computing softwar | Type: technology_name  | Raw: 0.53 | Boosted: 0.49 | Vec Norm: 6.62\n",
            "[DEBUG] Management information systems | Type: technology_name  | Raw: 0.52 | Boosted: 0.48 | Vec Norm: 6.28\n",
            "[DEBUG] Oracle Business Intelligence S | Type: technology_name  | Raw: 0.52 | Boosted: 0.47 | Vec Norm: 5.89\n",
            "[DEBUG] IBM InfoSphere DataStage (Data | Type: technology_name  | Raw: 0.52 | Boosted: 0.47 | Vec Norm: 5.54\n",
            "[DEBUG] Informatica Data Explorer (Dat | Type: technology_name  | Raw: 0.51 | Boosted: 0.47 | Vec Norm: 5.88\n",
            "[DEBUG] Qualys Platform (Qualys)       | Type: technology_name  | Raw: 0.50 | Boosted: 0.46 | Vec Norm: 5.90\n",
            "\n",
            "[DEBUG] Final user vector norm: 5.44\n",
            "\n",
            "[DEBUG] Top matches:\n",
            "  0.9538 - Data Warehousing Specialists\n",
            "  0.9531 - Database Architects\n",
            "  0.9525 - Business Intelligence Analysts\n",
            "  0.9517 - Database Administrators\n",
            "  0.9504 - Data Scientists\n",
            "Top 5 Recommended Jobs:\n",
            "- Data Warehousing Specialists (Score: 0.9538)\n",
            "- Database Architects (Score: 0.9531)\n",
            "- Business Intelligence Analysts (Score: 0.9525)\n",
            "- Database Administrators (Score: 0.9517)\n",
            "- Data Scientists (Score: 0.9504)\n"
          ]
        }
      ],
      "source": [
        "# Example usage with filtered_candidates from previous processing\n",
        "top_jobs = recommend_jobs(filtered_candidates)\n",
        "print(\"Top 5 Recommended Jobs:\")\n",
        "for job, score in top_jobs:\n",
        "    print(f\"- {job} (Score: {score:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032a8ab4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77dd2ba1",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tnsr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
